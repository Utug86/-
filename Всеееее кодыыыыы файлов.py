# –∫–æ–¥ - main.py

import asyncio
import sys
from pathlib import Path
from typing import Optional, Dict, Any, Set
import signal
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass, asdict
import json

from logs import get_logger
from rag_chunk_tracker import ChunkUsageTracker
from rag_retriever import HybridRetriever
from rag_telegram import TelegramPublisher
from rag_lmclient import LMClient
from rag_langchain_tools import enrich_context_with_tools
from rag_prompt_utils import get_prompt_parts
from image_utils import prepare_media_for_post, get_media_type

@dataclass
class SystemStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
    total_topics: int = 0
    processed_topics: int = 0
    failed_topics: int = 0
    start_time: Optional[datetime] = None
    current_topic: Optional[str] = None
    last_error: Optional[str] = None
    last_processing_time: Optional[float] = None
    total_chars_generated: int = 0
    avg_chars_per_topic: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        stats = asdict(self)
        if self.start_time:
            stats['start_time'] = self.start_time.isoformat()
            stats['running_time'] = str(datetime.now() - self.start_time)
        stats['success_rate'] = (
            (self.processed_topics / self.total_topics * 100)
            if self.total_topics > 0 else 0
        )
        return stats

class RAGException(Exception):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏–π RAG —Å–∏—Å—Ç–µ–º—ã"""
    pass

class ConfigurationError(RAGException):
    """–û—à–∏–±–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    pass

class InitializationError(RAGException):
    """–û—à–∏–±–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    pass

class ProcessingError(RAGException):
    """–û—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    pass

class RAGSystem:
    def __init__(self):
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.base_dir = Path(__file__).parent
        self.setup_paths()
        self.logger = get_logger(__name__, logfile=self.log_dir / "bot.log")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = SystemStats()
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        self._processed_topics: Set[str] = set()
        
        # –§–ª–∞–≥ –¥–ª—è graceful shutdown
        self.should_exit = False
        signal.signal(signal.SIGINT, self.handle_shutdown)
        signal.signal(signal.SIGTERM, self.handle_shutdown)

    def setup_paths(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.data_dir = self.base_dir / "data"
        self.log_dir = self.base_dir / "logs"
        self.inform_dir = self.base_dir / "inform"
        self.config_dir = self.base_dir / "config"
        self.media_dir = self.base_dir / "media"
        
        # –í–∞–∂–Ω—ã–µ —Ñ–∞–π–ª—ã
        self.topics_file = self.data_dir / "topics.txt"
        self.processed_topics_file = self.data_dir / "processed_topics.txt"
        self.index_file = self.data_dir / "faiss_index.idx"
        self.context_file = self.data_dir / "faiss_contexts.json"
        self.usage_stats_file = self.data_dir / "usage_statistics.json"

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        required_dirs = [
            self.data_dir,
            self.log_dir,
            self.inform_dir,
            self.config_dir,
            self.media_dir,
            self.data_dir / "prompt_1",
            self.data_dir / "prompt_2"
        ]
        
        for directory in required_dirs:
            try:
                directory.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                raise InitializationError(f"Failed to create directory {directory}: {e}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if not self.topics_file.exists():
            raise ConfigurationError("topics.txt not found")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–º –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.processed_topics_file.touch(exist_ok=True)

    def load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤
            token_file = self.config_dir / "telegram_token.txt"
            channel_file = self.config_dir / "telegram_channel.txt"

            if not token_file.exists():
                raise ConfigurationError("telegram_token.txt not found")
            if not channel_file.exists():
                raise ConfigurationError("telegram_channel.txt not found")

            # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            bot_token = token_file.read_text(encoding="utf-8").strip()
            channel_id = channel_file.read_text(encoding="utf-8").strip()

            if not bot_token or not channel_id:
                raise ConfigurationError(
                    "Telegram token or channel ID is empty"
                )

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            return {
                "telegram": {
                    "bot_token": bot_token,
                    "channel_id": channel_id,
                    "retry_attempts": 3,
                    "retry_delay": 3.0,
                    "enable_preview": True
                },
                "llm": {
                    "model_url": "http://localhost:1234/v1/chat/completions",
                    "model_name": "gemma-3-27b-it-GGUF/gemma-3-27b-it-Q4_K_M.gguf",
                    "max_tokens": 4096,
                    "max_chars": 4096,
                    "max_chars_with_media": 1024,
                    "temperature": 0.7,
                    "timeout": 40,
                    "history_limit": 3
                },
                "rag": {
                    "chunk_usage_limit": 10,
                    "usage_reset_days": 7,
                    "diversity_boost": 0.3,
                    "emb_model": "all-MiniLM-L6-v2",
                    "cross_model": "cross-encoder/stsb-roberta-large",
                    "chunk_size": 500,
                    "overlap": 100,
                    "top_k_title": 2,
                    "top_k_faiss": 8,
                    "top_k_final": 3
                }
            }
        except Exception as e:
            raise ConfigurationError(f"Failed to load configuration: {e}")

    def load_processed_topics(self) -> Set[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–º"""
        try:
            if self.processed_topics_file.exists():
                topics = set(self.processed_topics_file.read_text(
                    encoding='utf-8').splitlines())
                self.logger.info(f"Loaded {len(topics)} processed topics")
                return topics
            return set()
        except Exception as e:
            self.logger.warning(f"Failed to load processed topics: {e}")
            return set()

    def save_processed_topic(self, topic: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Ç–µ–º—ã"""
        try:
            if topic not in self._processed_topics:
                with open(self.processed_topics_file, 'a', encoding='utf-8') as f:
                    f.write(f"{topic}\n")
                self._processed_topics.add(topic)
                self.logger.info(f"Topic '{topic}' marked as processed")
        except Exception as e:
            self.logger.error(f"Failed to save processed topic: {e}")

    async def notify_error(self, message: str):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –≤ Telegram"""
        if hasattr(self, 'telegram'):
            try:
                await self.telegram.send_text(
                    f"üö® RAG System Error:\n{message}"
                )
            except Exception as e:
                self.logger.error(f"Failed to send error notification: {e}")

    def handle_shutdown(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        self.logger.info("Received shutdown signal, cleaning up...")
        self.should_exit = True
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–µ—Ä–µ–¥ –≤—ã—Ö–æ–¥–æ–º
        try:
            stats = self.stats.to_dict()
            stats_file = self.log_dir / f"stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Statistics saved to {stats_file}")
        except Exception as e:
            self.logger.error(f"Failed to save statistics: {e}")

    def _load_remaining_topics(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            all_topics = self.topics_file.read_text(encoding='utf-8').splitlines()
            remaining = [t for t in all_topics if t not in self._processed_topics]
            self.logger.info(f"Loaded {len(remaining)} remaining topics")
            return remaining
        except Exception as e:
            raise ProcessingError(f"Failed to load topics: {e}")

    async def process_topics(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–º –∏–∑ topics.txt"""
        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–º
        self._processed_topics = self.load_processed_topics()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–µ–º
        topics = self._load_remaining_topics()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats.total_topics = len(topics)
        self.stats.start_time = datetime.now()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–º
        for topic in topics:
            if self.should_exit:
                break

            self.stats.current_topic = topic
            processing_start = datetime.now()

            try:
                self.logger.info(
                    f"Processing topic {self.stats.processed_topics + 1}/{self.stats.total_topics}: {topic}"
                )
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–º—ã
                text_length = await self.process_single_topic(topic)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                self.stats.processed_topics += 1
                self.stats.total_chars_generated += text_length
                self.stats.avg_chars_per_topic = (
                    self.stats.total_chars_generated / self.stats.processed_topics
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                self.save_processed_topic(topic)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                self.stats.last_processing_time = (
                    datetime.now() - processing_start
                ).total_seconds()
                
            except Exception as e:
                error_msg = f"Error processing topic {topic}: {e}"
                self.logger.error(error_msg)
                self.stats.failed_topics += 1
                self.stats.last_error = error_msg
                await self.notify_error(error_msg)
                await asyncio.sleep(5)

    async def process_single_topic(self, topic: str) -> int:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Ç–µ–º—ã.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ RAG
            context = self.retriever.retrieve(topic)
            
            # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
            context = enrich_context_with_tools(topic, context, self.inform_dir)
            
            # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤
            prompt1_files = sorted((self.data_dir / "prompt_1").glob("*.txt"))
            prompt2_files = sorted((self.data_dir / "prompt_2").glob("*.txt"))
            
            if not prompt1_files or not prompt2_files:
                raise ProcessingError("No prompt files found")
            
            # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤
            import random
            file1 = random.choice(prompt1_files)
            file2 = random.choice(prompt2_files)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
            prompt_full = get_prompt_parts(
                data_dir=self.data_dir,
                topic=topic,
                context=context,
                file1=file1,
                file2=file2
            )

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
            max_chars = (
                self.config["llm"]["max_chars_with_media"]
                if "{UPLOADFILE}" in prompt_full
                else self.config["llm"]["max_chars"]
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            text = await self.lm.generate(
                topic,
                max_chars=max_chars
            )
            
            if not text:
                raise ProcessingError("Failed to generate text")

            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram
            if "{UPLOADFILE}" in prompt_full:
                await self.handle_media_post(text)
            else:
                await self.telegram.send_text(text)

            self.logger.info(
                f"Successfully processed topic: {topic}, "
                f"text length: {len(text)}"
            )
            
            return len(text)

        except Exception as e:
            raise ProcessingError(f"Failed to process topic {topic}: {e}")

    async def handle_media_post(self, text: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å—Ç–∞ —Å –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–º"""
        try:
            media_file = prepare_media_for_post(self.media_dir)
            if not media_file:
                raise ProcessingError("No valid media file found")

            media_type = get_media_type(media_file)
            self.logger.info(f"Selected media file: {media_file} (type: {media_type})")

            media_handlers = {
                "image": self.telegram.send_photo,
                "video": self.telegram.send_video,
                "document": self.telegram.send_document,
                "audio": self.telegram.send_audio
            }

            if media_type in media_handlers:
                await media_handlers[media_type](media_file, caption=text)
            else:
                self.logger.warning(f"Unknown media type: {media_file}")
                await self.telegram.send_text(text)

        except Exception as e:
            self.logger.error(f"Media handling error: {e}")
            await self.telegram.send_text(text)

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.config = self.load_config()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            self.usage_tracker = ChunkUsageTracker(
                usage_stats_file=self.usage_stats_file,
                logger=self.logger,
                chunk_usage_limit=self.config["rag"]["chunk_usage_limit"],
                usage_reset_days=self.config["rag"]["usage_reset_days"],
                diversity_boost=self.config["rag"]["diversity_boost"]
            )
            self.usage_tracker.cleanup_old_stats()

            self.retriever = HybridRetriever(
                emb_model=self.config["rag"]["emb_model"],
                cross_model=self.config["rag"]["cross_model"],
                index_file=self.index_file,
                context_file=self.context_file,
                inform_dir=self.inform_dir,
                chunk_size=self.config["rag"]["chunk_size"],
                overlap=self.config["rag"]["overlap"],
                top_k_title=self.config["rag"]["top_k_title"],
                top_k_faiss=self.config["rag"]["top_k_faiss"],
                top_k_final=self.config["rag"]["top_k_final"],
                usage_tracker=self.usage_tracker,
                logger=self.logger
            )

            self.lm = LMClient(
                retriever=self.retriever,
                data_dir=self.data_dir,
                inform_dir=self.inform_dir,
                logger=self.logger,
                model_url=self.config["llm"]["model_url"],
                model_name=self.config["llm"]["model_name"],
                max_tokens=self.config["llm"]["max_tokens"],
                max_chars=self.config["llm"]["max_chars"],
                temperature=self.config["llm"]["temperature"],
                timeout=self.config["llm"]["timeout"],
                history_lim=self.config["llm"]["history_limit"]
            )

            self.telegram = TelegramPublisher(
                self.config["telegram"]["bot_token"],
                self.config["telegram"]["channel_id"],
                logger=self.logger,
                max_retries=self.config["telegram"]["retry_attempts"],
                retry_delay=self.config["telegram"]["retry_delay"],
                enable_preview=self.config["telegram"]["enable_preview"]
            )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Telegram
            if not await self.telegram.check_connection():
                raise TelegramError("Failed to connect to Telegram")

            self.logger.info("System initialized successfully")
            
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏
            await self.process_topics()

        except ConfigurationError as e:
            self.logger.critical(f"Configuration error: {e}")
            await self.notify_error(f"Configuration error: {e}")
            sys.exit(1)
        except Exception as e:
            self.logger.critical(f"Unexpected error: {e}")
            await self.notify_error(f"Unexpected error: {e}")
            sys.exit(1)
        finally:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –æ—á–∏—Å—Ç–∫–∞
            if hasattr(self, 'usage_tracker'):
                self.usage_tracker.save_statistics()
            
            # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats = self.stats.to_dict()
            self.logger.info("Final statistics:")
            for key, value in stats.items():
                self.logger.info(f"{key}: {value}")
            
            self.logger.info("System shutdown complete")

def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—à–∏–±–æ–∫"""
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logging.error(
            "Uncaught exception",
            exc_info=(exc_type, exc_value, exc_traceback)
        )

    sys.excepthook = handle_exception
    
    # –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã
    rag_system = RAGSystem()
    try:
        asyncio.run(rag_system.run())
    except KeyboardInterrupt:
        print("\nShutdown requested... exiting")
    except Exception as e:
        print(f"Fatal error: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()

# –∫–æ–¥ - logs.py

import logging

def get_logger(name: str, logfile: str = None, level=logging.INFO) -> logging.Logger:
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        if logfile:
            fh = logging.FileHandler(logfile, encoding="utf-8")
            fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
            logger.addHandler(fh)
        sh = logging.StreamHandler()
        sh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
        logger.addHandler(sh)
        logger.setLevel(level)
    return logger

# –∫–æ–¥ - rag_file_utils.py

from pathlib import Path
import logging
import pandas as pd
from bs4 import BeautifulSoup
from logs import get_logger

# --- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ª–æ–≥–≥–µ—Ä ---
logger = get_logger("rag_file_utils")

def _try_import_docx():
    try:
        import docx
        return docx
    except ImportError:
        logger.warning("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –§–æ—Ä–º–∞—Ç—ã .docx –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
        return None

def _try_import_pypdf2():
    try:
        import PyPDF2
        return PyPDF2
    except ImportError:
        logger.warning("PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –§–æ—Ä–º–∞—Ç—ã .pdf –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
        return None

def _try_import_textract():
    try:
        import textract
        return textract
    except ImportError:
        logger.warning("textract –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω—ã.")
        return None

DOCX = _try_import_docx()
PDF = _try_import_pypdf2()
TEXTRACT = _try_import_textract()

# --- –ö–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–∞ ---
COMMON_ENCODINGS = ["utf-8", "cp1251", "windows-1251", "latin-1", "iso-8859-1"]

def _smart_read_text(path: Path) -> str:
    """
    –ß–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, –ø—Ä–æ–±—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫.
    """
    for encoding in COMMON_ENCODINGS:
        try:
            return path.read_text(encoding=encoding)
        except Exception as e:
            logger.debug(f"–ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding} –¥–ª—è {path}: {e}")
    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {path} –≤ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∫–∞—Ö: {COMMON_ENCODINGS}")
    return ""

def extract_text_from_file(path: Path) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: txt, html, csv, xlsx, xlsm, docx, doc, pdf.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    ext = path.suffix.lower()
    if not path.exists():
        logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        return ""

    try:
        if ext == ".txt":
            logger.info(f"Extracting text from TXT file: {path}")
            return _smart_read_text(path)

        elif ext == ".html":
            logger.info(f"Extracting text from HTML file: {path}")
            html_content = _smart_read_text(path)
            soup = BeautifulSoup(html_content, "html.parser")
            return soup.get_text(separator=" ")

        elif ext == ".csv":
            logger.info(f"Extracting text from CSV file: {path}")
            try:
                df = pd.read_csv(path)
                return df.to_csv(sep="\t", index=False)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV —á–µ—Ä–µ–∑ pandas: {e}. –ü—Ä–æ–±—É–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç.")
                return _smart_read_text(path)

        elif ext in [".xlsx", ".xls", ".xlsm"]:
            logger.info(f"Extracting text from Excel file: {path}")
            try:
                df = pd.read_excel(path)
                return df.to_csv(sep="\t", index=False)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Excel —á–µ—Ä–µ–∑ pandas: {e}")
                return ""

        elif ext == ".docx":
            logger.info(f"Extracting text from DOCX file: {path}")
            if DOCX is not None:
                try:
                    doc = DOCX.Document(path)
                    return "\n".join([p.text for p in doc.paragraphs])
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX: {e}")
                    return ""
            else:
                logger.warning(f"–ú–æ–¥—É–ª—å python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. DOCX –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")
                return ""

        elif ext == ".doc":
            logger.info(f"Extracting text from DOC file: {path}")
            if TEXTRACT is not None:
                try:
                    return TEXTRACT.process(str(path)).decode("utf-8", errors="ignore")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOC —á–µ—Ä–µ–∑ textract: {e}")
                    return ""
            else:
                logger.warning(f"textract –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. DOC –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")
                return ""

        elif ext == ".pdf":
            logger.info(f"Extracting text from PDF file: {path}")
            # –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ PyPDF2, –∑–∞—Ç–µ–º textract
            if PDF is not None:
                try:
                    text = []
                    with open(path, "rb") as f:
                        reader = PDF.PdfReader(f)
                        for page in reader.pages:
                            page_text = page.extract_text()
                            if page_text:
                                text.append(page_text)
                    return "\n".join(text)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF —á–µ—Ä–µ–∑ PyPDF2: {e}. –ü—Ä–æ–±—É–µ–º textract.")
            if TEXTRACT is not None:
                try:
                    return TEXTRACT.process(str(path)).decode("utf-8", errors="ignore")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF —á–µ—Ä–µ–∑ textract: {e}")
                    return ""
            logger.warning(f"PyPDF2 –∏ textract –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. PDF –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")
            return ""

        else:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {path}")
            return f"[–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {ext}]"

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {path}: {e}")
        return ""

def clean_html_from_cell(cell_value) -> str:
    """
    –û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É/—è—á–µ–π–∫—É –æ—Ç HTML-—Ç–µ–≥–æ–≤.
    """
    if isinstance(cell_value, str):
        return BeautifulSoup(cell_value, "html.parser").get_text(separator=" ")
    return str(cell_value)

# –∫–æ–¥ - rag_chunk_tracker.py

import json
import hashlib
from collections import Counter, deque
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Callable, Optional, Tuple

class ChunkUsageTracker:
    """
    –¢—Ä–µ–∫–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤ ‚Äî —Ö—Ä–∞–Ω–∏—Ç, –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π/–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —á–∞–Ω–∫–æ–≤ (hash —á–∞–Ω–∫–∞), –≤–µ—Ä—Å–∏–æ–Ω–Ω–æ—Å—Ç—å –±–∞–∑—ã, –≥–∏–±–∫–∏–µ penalty/boost —Ñ—É–Ω–∫—Ü–∏–∏,
    –ø–æ–ª–Ω—É—é –æ—á–∏—Å—Ç–∫—É –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
    """

    def __init__(
        self,
        usage_stats_file: Path,
        logger,
        chunk_usage_limit: int,
        usage_reset_days: int,
        diversity_boost: float,
        index_version: Optional[str] = None,
        index_hash: Optional[str] = None,
        penalty_func: Optional[Callable[[int, int, int], float]] = None,
        boost_func: Optional[Callable[[int, int], float]] = None,
    ):
        """
        :param usage_stats_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        :param logger: –õ–æ–≥–≥–µ—Ä
        :param chunk_usage_limit: –õ–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è penalty
        :param usage_reset_days: –°–∫–æ–ª—å–∫–æ –¥–Ω–µ–π —Ö—Ä–∞–Ω–∏—Ç—å usage –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π
        :param diversity_boost: –ë–∞–∑–æ–≤—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç diversity
        :param index_version: –í–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞/–±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (–¥–ª—è —Å–±—Ä–æ—Å–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏)
        :param index_hash: –•–µ—à –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (–¥–ª—è —Å–±—Ä–æ—Å–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏)
        :param penalty_func: –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è penalty, –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (chunk_count, title_count, chunk_usage_limit)
        :param boost_func: –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è diversity boost, –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (chunk_count, chunk_usage_limit)
        """
        self.usage_stats_file: Path = usage_stats_file
        self.logger = logger
        self.chunk_usage_limit = chunk_usage_limit
        self.usage_reset_days = usage_reset_days
        self.diversity_boost = diversity_boost
        self.index_version = index_version
        self.index_hash = index_hash

        self.penalty_func = penalty_func or self._default_penalty_func
        self.boost_func = boost_func or self._default_boost_func

        self.usage_stats: Dict[str, Dict[str, Any]] = {}
        self.recent_usage: deque = deque(maxlen=100)
        self.title_usage: Counter = Counter()
        self.chunk_usage: Counter = Counter()
        self.session_usage: Counter = Counter()
        self.loaded_index_version = None
        self.loaded_index_hash = None
        self.load_statistics()

    @staticmethod
    def get_chunk_hash(chunk_text: str, source: Optional[str]=None) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —á–∞–Ω–∫–∞ (sha1 –æ—Ç —Ç–µ–∫—Å—Ç–∞ + source).
        """
        to_hash = (chunk_text or "") + "|" + (source or "")
        return hashlib.sha1(to_hash.encode('utf-8')).hexdigest()

    def load_statistics(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ —Ñ–∞–π–ª–∞. –ï—Å–ª–∏ –≤–µ—Ä—Å–∏—è/—Ö–µ—à –±–∞–∑—ã –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è ‚Äî —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç usage.
        """
        try:
            if self.usage_stats_file.exists():
                with open(self.usage_stats_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.usage_stats = data.get('usage_stats', {})
                    self.title_usage = Counter(data.get('title_usage', {}))
                    self.chunk_usage = Counter(data.get('chunk_usage', {}))
                    self.recent_usage = deque(data.get('recent_usage', []), maxlen=100)
                    self.loaded_index_version = data.get('index_version')
                    self.loaded_index_hash = data.get('index_hash')
                if (self.index_version and self.loaded_index_version != self.index_version) or \
                   (self.index_hash and self.loaded_index_hash != self.index_hash):
                    self.logger.warning("Knowledge base index version/hash mismatch: usage statistics will be reset.")
                    self.clear_all_statistics()
                else:
                    self.logger.info("Loaded usage statistics successfully")
        except Exception as e:
            self.logger.warning(f"Failed to load usage statistics: {e}")
            self.usage_stats = {}

    def save_statistics(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª (atomic save).
        """
        try:
            data = {
                'usage_stats': self.usage_stats,
                'title_usage': dict(self.title_usage),
                'chunk_usage': dict(self.chunk_usage),
                'recent_usage': list(self.recent_usage),
                'last_updated': datetime.now().isoformat(),
                'index_version': self.index_version,
                'index_hash': self.index_hash
            }
            tmp_file = self.usage_stats_file.with_suffix('.tmp')
            with open(tmp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            tmp_file.replace(self.usage_stats_file)
        except Exception as e:
            self.logger.error(f"Failed to save usage statistics: {e}")

    def record_usage(self, chunk_hashes: List[str], titles: List[str], metadata: List[Dict[str, Any]]):
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (–ø–æ hash'–∞–º).
        :param chunk_hashes: –°–ø–∏—Å–æ–∫ —Ö–µ—à–µ–π —á–∞–Ω–∫–æ–≤ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ id)
        :param titles: –°–ø–∏—Å–æ–∫ —Ç–∞–π—Ç–ª–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç chunk_hashes)
        :param metadata: –ú–∞—Å—Å–∏–≤ –º–µ—Ç–∞–¥–∞—Ç—ã (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
        """
        timestamp = datetime.now().isoformat()
        for i, chunk_hash in enumerate(chunk_hashes):
            title = titles[i] if i < len(titles) else "unknown"
            if chunk_hash not in self.usage_stats:
                self.usage_stats[chunk_hash] = {
                    'count': 0,
                    'last_used': None,
                    'title': title
                }
            self.usage_stats[chunk_hash]['count'] += 1
            self.usage_stats[chunk_hash]['last_used'] = timestamp
            self.title_usage[title] += 1
            self.chunk_usage[chunk_hash] += 1
            self.session_usage[chunk_hash] += 1
            self.recent_usage.append({
                'chunk_id': chunk_hash,
                'title': title,
                'timestamp': timestamp
            })
        self.save_statistics()

    def get_usage_penalty(self, chunk_hash: str, title: str) -> float:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ —á–∞—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (0.0 - 1.5), —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π penalty-—Ñ—É–Ω–∫—Ü–∏–∏.
        """
        chunk_count = self.usage_stats.get(chunk_hash, {}).get('count', 0)
        title_count = self.title_usage.get(title, 0)
        return self.penalty_func(chunk_count, title_count, self.chunk_usage_limit)

    def get_diversity_boost(self, chunk_hash: str, title: str) -> float:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±—É—Å—Ç –¥–ª—è —Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —á–∞–Ω–∫–æ–≤, —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π boost-—Ñ—É–Ω–∫—Ü–∏–∏.
        """
        chunk_count = self.usage_stats.get(chunk_hash, {}).get('count', 0)
        return self.boost_func(chunk_count, self.chunk_usage_limit)

    @staticmethod
    def _default_penalty_func(chunk_count: int, title_count: int, chunk_usage_limit: int) -> float:
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è penalty-—Ñ—É–Ω–∫—Ü–∏—è (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ __init__).
        """
        chunk_penalty = min(chunk_count / chunk_usage_limit, 1.0)
        title_penalty = min(title_count / (chunk_usage_limit * 2), 0.5)
        return chunk_penalty + title_penalty

    @staticmethod
    def _default_boost_func(chunk_count: int, chunk_usage_limit: int) -> float:
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è diversity boost (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ __init__).
        """
        if chunk_count == 0:
            return 2.0
        elif chunk_count < chunk_usage_limit // 3:
            return 1.0
        else:
            return 0.0

    def cleanup_old_stats(self, full_reset: bool = False):
        """
        –û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –ª–∏–±–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç usage –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É.
        :param full_reset: –ï—Å–ª–∏ True ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç usage —É –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤, last_used –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞—Ä—à–µ –ø–æ—Ä–æ–≥–∞.
        """
        cutoff_date = datetime.now() - timedelta(days=self.usage_reset_days)
        cutoff_str = cutoff_date.isoformat()
        cleaned_count = 0
        for chunk_hash in list(self.usage_stats.keys()):
            last_used = self.usage_stats[chunk_hash].get('last_used')
            if last_used and last_used < cutoff_str:
                if full_reset:
                    del self.usage_stats[chunk_hash]
                    cleaned_count += 1
                else:
                    old_count = self.usage_stats[chunk_hash]['count']
                    self.usage_stats[chunk_hash]['count'] = max(0, old_count - 1)
                    if self.usage_stats[chunk_hash]['count'] == 0:
                        del self.usage_stats[chunk_hash]
                        cleaned_count += 1
        if cleaned_count > 0:
            self.logger.info(f"Cleaned up {cleaned_count} old usage statistics entries{' (full reset)' if full_reset else ''}")
            self.save_statistics()

    def clear_all_statistics(self):
        """
        –ü–æ–ª–Ω—ã–π —Å–±—Ä–æ—Å –≤—Å–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
        """
        self.usage_stats.clear()
        self.title_usage.clear()
        self.chunk_usage.clear()
        self.session_usage.clear()
        self.recent_usage.clear()
        self.save_statistics()
        self.logger.info("All usage statistics cleared.")

    def get_unused_chunks(self, metadata: List[Dict[str, Any]]) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ hash'–µ–π –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ metadata.
        """
        all_hashes = set(self.get_chunk_hash(m['chunk'], m.get('source')) for m in metadata)
        used = set(self.usage_stats.keys())
        return list(all_hashes - used)

    def get_top_used_chunks(self, n: int = 10) -> List[Tuple[str, int]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —á–∞–Ω–∫–æ–≤ (hash, count).
        """
        return self.chunk_usage.most_common(n)

    def get_usage_distribution(self) -> Dict[str, int]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ —á–∞–Ω–∫–∞–º (hash -> count).
        """
        return dict(self.chunk_usage)

    def get_stats_summary(self) -> Dict[str, Any]:
        """
        –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
        """
        total_chunks = len(self.usage_stats)
        total_titles = len(self.title_usage)
        never_used = sum(1 for data in self.usage_stats.values() if data.get('count', 0) == 0)
        most_used = self.chunk_usage.most_common(1)[0] if self.chunk_usage else ("", 0)
        return {
            "total_chunks": total_chunks,
            "total_titles": total_titles,
            "never_used_chunks": never_used,
            "most_used_chunk": most_used,
        }

    # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è property-–º–µ—Ç–æ–¥–æ–≤
    @property
    def usage_stats_count(self) -> int:
        """–¢–µ–∫—É—â–µ–µ —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å usage-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."""
        return len(self.usage_stats)

    @property
    def title_count(self) -> int:
        """–ß–∏—Å–ª–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (title) –≤ usage-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ."""
        return len(self.title_usage)

# –∫–æ–¥ - rag_retriever.py

import faiss
import numpy as np
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer, CrossEncoder
from typing import Any, Dict, List, Optional
import hashlib
import logging
import datetime
import itertools

from rag_utils import extract_text_from_file

def notify_admin(message: str):
    # –ü—Ä–∏–º–µ—Ä: –æ—Ç–ø—Ä–∞–≤–∫–∞ email/–ª–æ–≥/–¥—Ä—É–≥–æ–π —Å–∏—Å—Ç–µ–º—ã –æ–ø–æ–≤–µ—â–µ–Ω–∏—è
    logging.warning(f"[ADMIN NOTIFY] {message}")

class HybridRetriever:
    INDEX_VERSION = "1.2"  # –û–±–Ω–æ–≤–ª–µ–Ω–æ: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ –¥–æ–ø. –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

    def __init__(
        self,
        emb_model: str,
        cross_model: str,
        index_file: Path,
        context_file: Path,
        inform_dir: Path,
        chunk_size: int,
        overlap: int,
        top_k_title: int,
        top_k_faiss: int,
        top_k_final: int,
        usage_tracker,
        logger
    ):
        self.emb_model = emb_model
        self.cross_model = cross_model
        self.index_file = Path(index_file)
        self.context_file = Path(context_file)
        self.inform_dir = Path(inform_dir)
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.top_k_title = top_k_title
        self.top_k_faiss = top_k_faiss
        self.top_k_final = top_k_final
        self.usage_tracker = usage_tracker
        self.logger = logger

        self.sentencemodel = SentenceTransformer(self.emb_model)
        self.crossencoder = CrossEncoder(self.cross_model)
        self.faiss_index = None
        self.metadata = None
        self.index_metadata = {}

        self._try_load_or_build_indices()

    def _get_index_signature(self):
        conf = {
            "version": self.INDEX_VERSION,
            "emb_model": self.emb_model,
            "cross_model": self.cross_model,
            "chunk_size": self.chunk_size,
            "overlap": self.overlap,
            "inform_dir_hash": self._dir_hash(self.inform_dir)
        }
        return hashlib.sha256(json.dumps(conf, sort_keys=True).encode()).hexdigest()

    @staticmethod
    def _dir_hash(directory: Path) -> str:
        if not directory.exists():
            return ""
        files = sorted([(str(f), f.stat().st_mtime) for f in directory.iterdir() if f.is_file()])
        return hashlib.sha256(json.dumps(files, sort_keys=True).encode()).hexdigest()

    def _try_load_or_build_indices(self):
        self.logger.info("Initializing HybridRetriever...")
        rebuild_needed = False
        if self.index_file.exists() and self.context_file.exists():
            try:
                self._load_indices()
                idx_sig = self.index_metadata.get("index_signature")
                expected_sig = self._get_index_signature()
                if idx_sig != expected_sig:
                    self.logger.warning("Index signature mismatch (model/chunking/config changed). Rebuilding index...")
                    notify_admin("HybridRetriever: Index signature mismatch, forced rebuild triggered.")
                    rebuild_needed = True
            except Exception as e:
                self.logger.warning(f"Failed to load indices: {e}. Rebuilding...")
                notify_admin(f"HybridRetriever: Failed to load indices: {e}. Forced rebuild triggered.")
                rebuild_needed = True
        else:
            self.logger.info("No existing indices found. Building new ones...")
            rebuild_needed = True
        if rebuild_needed:
            self._build_indices()

    def _load_indices(self):
        self.logger.info("Loading indices...")
        try:
            with open(self.context_file, 'r', encoding='utf-8') as f:
                context_json = json.load(f)
                if isinstance(context_json, dict) and "metadata" in context_json:
                    self.metadata = context_json["metadata"]
                    self.index_metadata = context_json.get("index_metadata", {})
                else:
                    self.metadata = context_json
                    self.index_metadata = {}
            if not isinstance(self.metadata, list) or len(self.metadata) == 0:
                raise ValueError("Metadata must be a non-empty list")
            sample = self.metadata[0]
            required_fields = ['title', 'chunk']
            if not all(field in sample for field in required_fields):
                raise ValueError(f"Metadata must contain fields: {required_fields}")
            for item in self.metadata:
                if 'tokens' not in item:
                    item['tokens'] = item['chunk'].split()
                if isinstance(item['tokens'], str):
                    item['tokens'] = item['tokens'].split()
                if 'created_at' not in item:
                    item['created_at'] = None
                if 'source' not in item:
                    item['source'] = None
            self.faiss_index = faiss.read_index(str(self.index_file))
            self.logger.info(f"Loaded {len(self.metadata)} chunks")
            self.logger.info(f"Index metadata: {self.index_metadata}")
        except Exception as e:
            self.logger.error(f"Error loading indices: {e}")
            notify_admin(f"HybridRetriever: Error loading indices: {e}")
            raise

    def _normalize_text(self, text: str) -> str:
        """
        –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:
        - –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
        - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        - –£–¥–∞–ª–µ–Ω–∏–µ html-—Ç–µ–≥–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        - –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∫—Ä–æ–º–µ –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        """
        import re
        from bs4 import BeautifulSoup
        text = BeautifulSoup(text, "html.parser").get_text(separator=" ")
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^–∞-—èa-z0-9\s\.,:;!\?\(\)\[\]\'\"-]', '', text)
        text = text.strip()
        return text

    def _semantic_deduplicate(self, chunks: List[Dict], threshold: float = 0.91) -> List[Dict]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤, —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ cosine similarity.
        threshold: –µ—Å–ª–∏ cosine similarity > threshold, —Å—á–∏—Ç–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–º.
        """
        if len(chunks) < 2:
            return chunks
        texts = [c['chunk'] for c in chunks]
        embs = self.sentencemodel.encode(texts, convert_to_tensor=False, show_progress_bar=False, normalize_embeddings=True)
        embs = np.asarray(embs, dtype='float32')
        to_keep = []
        already_used = set()
        for i in range(len(chunks)):
            if i in already_used:
                continue
            to_keep.append(chunks[i])
            for j in range(i+1, len(chunks)):
                if j in already_used:
                    continue
                sim = np.dot(embs[i], embs[j])
                if sim > threshold:
                    already_used.add(j)
        deduped = to_keep
        self.logger.info(f"Semantic deduplication: {len(chunks)} ‚Üí {len(deduped)} unique chunks (threshold={threshold})")
        return deduped

    def _build_indices(self):
        self.logger.info("Building new indices...")
        metadata = []
        inform_files = [f for f in self.inform_dir.iterdir()
                        if f.suffix.lower() in [".txt", ".html", ".csv", ".xlsx", ".xlsm", ".doc", ".docx", ".pdf"]]
        if not inform_files:
            notify_admin(f"HybridRetriever: No suitable files found in {self.inform_dir}")
            raise RuntimeError(f"No suitable files found in {self.inform_dir}")
        self.logger.info(f"Found {len(inform_files)} files to process in inform")
        index_time = datetime.datetime.utcnow().isoformat()
        for file in inform_files:
            try:
                title = file.stem.lower()
                text = extract_text_from_file(file)
                if not text or not text.strip():
                    self.logger.warning(f"Empty or unreadable file: {file}")
                    continue
                # –°–ª–æ–∂–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
                text = self._normalize_text(text)
                words = text.split()
                chunks = []
                for i in range(0, len(words), self.chunk_size - self.overlap):
                    chunk = ' '.join(words[i:i + self.chunk_size])
                    if len(chunk.strip()) < 10:
                        continue
                    tokens = chunk.split()
                    # –ë–∞–∑–æ–≤–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å—Ç—Ä–æ–∫
                    if any(chunk == m['chunk'] for m in chunks):
                        continue
                    chunks.append({'title': title, 'chunk': chunk, 'tokens': tokens,
                                   'created_at': index_time, 'source': str(file)})
                if not chunks:
                    continue
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–∞–Ω–∫–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                deduped_chunks = self._semantic_deduplicate(chunks, threshold=0.91)
                metadata.extend(deduped_chunks)
                self.logger.info(f"Processed {file.name}: {len(words)} words -> {len(deduped_chunks)} unique chunks")
            except Exception as e:
                self.logger.error(f"Error processing file {file}: {e}")
                notify_admin(f"HybridRetriever: Error processing file {file}: {e}")
                continue
        if not metadata:
            notify_admin("HybridRetriever: No valid chunks created from files")
            raise RuntimeError("No valid chunks created from files")
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ (–ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º)
        metadata = self._semantic_deduplicate(metadata, threshold=0.91)
        self.logger.info(f"Total unique chunks after global deduplication: {len(metadata)}")
        try:
            texts = [f"{m['title']}: {m['chunk']}" for m in metadata]
            self.logger.info("Generating embeddings...")
            embs = self.sentencemodel.encode(texts, convert_to_tensor=False, show_progress_bar=True, batch_size=32, normalize_embeddings=True)
            embs = np.asarray(embs, dtype='float32')
            dim = embs.shape[1]
            if len(metadata) > 10000:
                self.logger.info("Large dataset detected. Using HNSW index for scalability.")
                self.faiss_index = faiss.IndexHNSWFlat(dim, 32)
            else:
                self.faiss_index = faiss.IndexFlatL2(dim)
            self.faiss_index.add(embs)
            faiss.write_index(self.faiss_index, str(self.index_file))
            self.index_metadata = {
                "index_signature": self._get_index_signature(),
                "version": self.INDEX_VERSION,
                "emb_model": self.emb_model,
                "cross_model": self.cross_model,
                "chunk_size": self.chunk_size,
                "overlap": self.overlap,
                "created_at": index_time,
                "num_chunks": len(metadata),
                "inform_dir": str(self.inform_dir),
            }
            context_json = {
                "metadata": metadata,
                "index_metadata": self.index_metadata
            }
            with open(self.context_file, 'w', encoding='utf-8') as f:
                json.dump(context_json, f, ensure_ascii=False, indent=2)
            self.metadata = metadata
            self.logger.info(f"Indices built and saved: {len(metadata)} unique chunks, index type: {type(self.faiss_index)}")
            self.logger.info(f"Index metadata: {self.index_metadata}")
            notify_admin(f"HybridRetriever: Index successfully rebuilt. {len(metadata)} unique chunks.")
        except Exception as e:
            self.logger.error(f"Error building or saving indices: {e}")
            notify_admin(f"HybridRetriever: Error building index: {e}")
            raise

    def retrieve(self, query: str, return_chunks: bool = False) -> str:
        self.logger.info(f"Retrieving context for query: '{query}'")
        if self.faiss_index is None or self.metadata is None or len(self.metadata) == 0:
            self.logger.error("Index not loaded or metadata is empty")
            notify_admin("HybridRetriever: Retrieval failed ‚Äî index not loaded or metadata is empty")
            return ""
        query_emb = self.sentencemodel.encode([query], convert_to_tensor=False, normalize_embeddings=True)
        query_emb = np.asarray(query_emb, dtype='float32')
        D, I = self.faiss_index.search(query_emb, self.top_k_faiss)
        I = I[0]
        D = D[0]
        if not len(I):
            self.logger.warning("No relevant chunks found in index")
            return ""
        candidates = []
        for idx, dist in zip(I, D):
            if idx < 0 or idx >= len(self.metadata):
                continue
            meta = self.metadata[idx].copy()
            meta['faiss_dist'] = float(dist)
            candidates.append(meta)
        for c in candidates:
            c['usage_penalty'] = self.usage_tracker.get_penalty(c['chunk']) if self.usage_tracker else 0.0
        candidates.sort(key=lambda x: (x['faiss_dist'] + x['usage_penalty']))
        rerank_candidates = candidates[:self.top_k_final * 2]
        ce_scores = []
        try:
            if self.crossencoder:
                pairs = [[query, c['chunk']] for c in rerank_candidates]
                ce_scores = self.crossencoder.predict(pairs)
                for c, score in zip(rerank_candidates, ce_scores):
                    c['cross_score'] = float(score)
                rerank_candidates.sort(key=lambda x: -x.get('cross_score', 0))
            else:
                for c in rerank_candidates:
                    c['cross_score'] = 0.0
        except Exception as e:
            self.logger.error(f"Cross-encoder rerank failed: {e}")
            for c in rerank_candidates:
                c['cross_score'] = 0.0
        selected = []
        titles = set()
        for c in rerank_candidates:
            if len(selected) >= self.top_k_final:
                break
            if c['title'] not in titles or self.top_k_final > len(rerank_candidates):
                selected.append(c)
                titles.add(c['title'])
        result = "\n\n".join([f"[{c['title']}] {c['chunk']}" for c in selected])
        self.logger.info(f"Retrieved {len(selected)} chunks from index for query '{query}'")
        if return_chunks:
            return selected
        return result

    def get_index_stats(self) -> Dict[str, Any]:
        stats = {
            "num_chunks": len(self.metadata) if self.metadata else 0,
            "num_files": len(set(m['source'] for m in self.metadata)) if self.metadata else 0,
            "unique_titles": len(set(m['title'] for m in self.metadata)) if self.metadata else 0,
            "index_type": type(self.faiss_index).__name__ if self.faiss_index else None,
            "index_metadata": self.index_metadata,
        }
        self.logger.info(f"Index stats: {stats}")
        return stats

    def rebuild_index(self):
        self.logger.info("Manual index rebuild triggered...")
        notify_admin("Manual HybridRetriever index rebuild triggered by user.")
        self._build_indices()

# –∫–æ–¥ - rag_table_utils.py

import pandas as pd
from pathlib import Path
from rag_file_utils import clean_html_from_cell
import logging

logger = logging.getLogger("rag_table_utils")
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] [rag_table_utils] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

def process_table_for_rag(
    file_path: Path,
    columns=None,
    filter_expr=None,
    add_headers=True,
    row_delim="\n"
) -> str:
    ext = file_path.suffix.lower()
    try:
        logger.info(f"Processing table for RAG: {file_path.name}")
        if ext == ".csv":
            df = pd.read_csv(file_path, usecols=columns)
        elif ext in [".xlsx", ".xls", ".xlsm"]:
            df = pd.read_excel(file_path, usecols=columns)
        else:
            logger.error("Not a table file")
            raise ValueError("Not a table file")
        if filter_expr:
            df = df.query(filter_expr)
        for col in df.columns:
            df[col] = df[col].apply(clean_html_from_cell)
        rows = []
        colnames = list(df.columns)
        for idx, row in df.iterrows():
            row_items = [f"{col}: {row[col]}" for col in colnames]
            rows.append(" | ".join(row_items))
        result = ""
        if add_headers:
            header = " | ".join(colnames)
            result = header + row_delim
        result += row_delim.join(rows)
        logger.info(f"Table processed for RAG: {file_path.name}, rows: {len(df)}")
        return result
    except Exception as e:
        logger.error(f"process_table_for_rag error: {e}")
        return f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è RAG]: {e}"

# –∫–æ–¥ - rag_telegram.py

import requests
import json
from pathlib import Path
from typing import Union, Optional, List
import logging
import time
import html

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è
logger = logging.getLogger("rag_telegram")
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] [rag_telegram] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

def escape_html(text: str) -> str:
    """
    –≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç HTML-—Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –¥–ª—è Telegram (HTML-mode).
    """
    return html.escape(text, quote=False)

class TelegramPublisher:
    """
    –ü—É–±–ª–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Ñ–∞–π–ª–æ–≤ –≤ Telegram-–∫–∞–Ω–∞–ª —á–µ—Ä–µ–∑ Bot API.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ, –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Å—ã–ª–æ–∫, –æ—Ç–ª–æ–∂–µ–Ω–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é.
    """

    def __init__(
        self,
        bot_token: str,
        channel_id: Union[str, int],
        logger: Optional[logging.Logger] = None,
        max_retries: int = 3,
        retry_delay: float = 3.0,
        enable_preview: bool = True
    ):
        """
        :param bot_token: –¢–æ–∫–µ–Ω Telegram-–±–æ—Ç–∞
        :param channel_id: ID –∏–ª–∏ username –∫–∞–Ω–∞–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, @my_channel)
        :param logger: –õ–æ–≥–≥–µ—Ä
        :param max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö —Å–µ—Ç–∏/Telegram
        :param retry_delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (—Å–µ–∫)
        :param enable_preview: –í–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Å—ã–ª–æ–∫ –≤ –ø–æ—Å—Ç–∞—Ö
        """
        self.bot_token = bot_token
        self.channel_id = channel_id
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.enable_preview = enable_preview
        self.logger = logger or logging.getLogger("rag_telegram")

    def _post(self, method: str, data: dict, files: dict = None) -> dict:
        url = f"https://api.telegram.org/bot{self.bot_token}/{method}"
        last_exc = None
        for attempt in range(1, self.max_retries + 1):
            try:
                resp = requests.post(url, data=data, files=files, timeout=20)
                resp.raise_for_status()
                result = resp.json()
                if not result.get("ok"):
                    self.logger.error(f"Telegram API error: {result}")
                    raise Exception(f"Telegram API error: {result}")
                return result
            except Exception as e:
                last_exc = e
                self.logger.warning(f"Telegram API request failed (attempt {attempt}): {e}")
                time.sleep(self.retry_delay)
        self.logger.error(f"Telegram API request failed after {self.max_retries} attempts: {last_exc}")
        raise last_exc

    def send_text(
        self,
        text: str,
        parse_mode: str = "HTML",
        disable_preview: Optional[bool] = None,
        reply_to_message_id: Optional[int] = None,
        silent: bool = False,
        html_escape: bool = True
    ) -> Optional[int]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∫–∞–Ω–∞–ª.
        :param html_escape: —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å HTML-—Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã (True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        :return: message_id –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ HTML –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
        if html_escape and parse_mode == "HTML":
            text = escape_html(text)
        data = {
            "chat_id": self.channel_id,
            "text": text,
            "parse_mode": parse_mode,
            "disable_web_page_preview": not (disable_preview if disable_preview is not None else self.enable_preview),
            "disable_notification": silent,
        }
        if reply_to_message_id:
            data["reply_to_message_id"] = reply_to_message_id
        try:
            resp = self._post("sendMessage", data)
            msg_id = resp.get("result", {}).get("message_id")
            self.logger.info(f"Message posted to Telegram (id={msg_id})")
            return msg_id
        except Exception as e:
            self.logger.error(f"Failed to send text message: {e}")
            return None

    def send_photo(
        self,
        photo: Union[str, Path],
        caption: Optional[str] = None,
        parse_mode: str = "HTML",
        disable_preview: Optional[bool] = None,
        reply_to_message_id: Optional[int] = None,
        silent: bool = False,
        html_escape: bool = True
    ) -> Optional[int]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ç–æ —Å –ø–æ–¥–ø–∏—Å—å—é.
        :param photo: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ URL
        :param html_escape: —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å HTML –≤ –ø–æ–¥–ø–∏—Å–∏
        :return: message_id –∏–ª–∏ None
        """
        data = {
            "chat_id": self.channel_id,
            "parse_mode": parse_mode,
            "disable_notification": silent,
        }
        if caption:
            data["caption"] = escape_html(caption) if html_escape and parse_mode == "HTML" else caption
        if reply_to_message_id:
            data["reply_to_message_id"] = reply_to_message_id
        files = {}
        if isinstance(photo, (str, Path)) and Path(photo).exists():
            files["photo"] = open(photo, "rb")
        else:
            data["photo"] = str(photo)
        try:
            resp = self._post("sendPhoto", data, files)
            msg_id = resp.get("result", {}).get("message_id")
            self.logger.info(f"Photo posted to Telegram (id={msg_id})")
            return msg_id
        except Exception as e:
            self.logger.error(f"Failed to send photo: {e}")
            return None
        finally:
            if files:
                files["photo"].close()

    def send_video(
        self,
        video: Union[str, Path],
        caption: Optional[str] = None,
        parse_mode: str = "HTML",
        reply_to_message_id: Optional[int] = None,
        silent: bool = False,
        html_escape: bool = True
    ) -> Optional[int]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞.
        """
        data = {
            "chat_id": self.channel_id,
            "parse_mode": parse_mode,
            "disable_notification": silent,
        }
        if caption:
            data["caption"] = escape_html(caption) if html_escape and parse_mode == "HTML" else caption
        if reply_to_message_id:
            data["reply_to_message_id"] = reply_to_message_id
        files = {}
        if isinstance(video, (str, Path)) and Path(video).exists():
            files["video"] = open(video, "rb")
        else:
            data["video"] = str(video)
        try:
            resp = self._post("sendVideo", data, files)
            msg_id = resp.get("result", {}).get("message_id")
            self.logger.info(f"Video posted to Telegram (id={msg_id})")
            return msg_id
        except Exception as e:
            self.logger.error(f"Failed to send video: {e}")
            return None
        finally:
            if files:
                files["video"].close()

    def send_audio(
        self,
        audio: Union[str, Path],
        caption: Optional[str] = None,
        parse_mode: str = "HTML",
        reply_to_message_id: Optional[int] = None,
        silent: bool = False,
        html_escape: bool = True
    ) -> Optional[int]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞.
        """
        data = {
            "chat_id": self.channel_id,
            "parse_mode": parse_mode,
            "disable_notification": silent,
        }
        if caption:
            data["caption"] = escape_html(caption) if html_escape and parse_mode == "HTML" else caption
        if reply_to_message_id:
            data["reply_to_message_id"] = reply_to_message_id
        files = {}
        if isinstance(audio, (str, Path)) and Path(audio).exists():
            files["audio"] = open(audio, "rb")
        else:
            data["audio"] = str(audio)
        try:
            resp = self._post("sendAudio", data, files)
            msg_id = resp.get("result", {}).get("message_id")
            self.logger.info(f"Audio posted to Telegram (id={msg_id})")
            return msg_id
        except Exception as e:
            self.logger.error(f"Failed to send audio: {e}")
            return None
        finally:
            if files:
                files["audio"].close()

    def send_document(
        self,
        document: Union[str, Path],
        caption: Optional[str] = None,
        parse_mode: str = "HTML",
        reply_to_message_id: Optional[int] = None,
        silent: bool = False,
        html_escape: bool = True
    ) -> Optional[int]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∞–π–ª–∞-–¥–æ–∫—É–º–µ–Ω—Ç–∞.
        :param document: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ URL
        :param html_escape: —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å HTML –≤ –ø–æ–¥–ø–∏—Å–∏
        :return: message_id –∏–ª–∏ None
        """
        data = {
            "chat_id": self.channel_id,
            "parse_mode": parse_mode,
            "disable_notification": silent,
        }
        if caption:
            data["caption"] = escape_html(caption) if html_escape and parse_mode == "HTML" else caption
        if reply_to_message_id:
            data["reply_to_message_id"] = reply_to_message_id
        files = {}
        if isinstance(document, (str, Path)) and Path(document).exists():
            files["document"] = open(document, "rb")
        else:
            data["document"] = str(document)
        try:
            resp = self._post("sendDocument", data, files)
            msg_id = resp.get("result", {}).get("message_id")
            self.logger.info(f"Document posted to Telegram (id={msg_id})")
            return msg_id
        except Exception as e:
            self.logger.error(f"Failed to send document: {e}")
            return None
        finally:
            if files:
                files["document"].close()

    def send_media_group(
        self,
        media: List[dict]
    ) -> Optional[List[int]]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞–±–æ—Ä–∞ –º–µ–¥–∏–∞ (—Ñ–æ—Ç–æ/–≤–∏–¥–µ–æ) –≤ –æ–¥–Ω–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏.
        :param media: —Å–ø–∏—Å–æ–∫ dict —Å —Ç–∏–ø–æ–º ('photo'/'video'), media (file_id/url), caption (optional)
        :return: —Å–ø–∏—Å–æ–∫ message_id –∏–ª–∏ None
        """
        data = {
            "chat_id": self.channel_id,
            "media": json.dumps(media, ensure_ascii=False),
        }
        try:
            resp = self._post("sendMediaGroup", data)
            results = resp.get("result", [])
            msg_ids = [msg.get("message_id") for msg in results if "message_id" in msg]
            self.logger.info(f"Media group posted to Telegram (messages={msg_ids})")
            return msg_ids
        except Exception as e:
            self.logger.error(f"Failed to send media group: {e}")
            return None

    def check_connection(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–∏ —Å Telegram Bot API (getMe).
        """
        url = f"https://api.telegram.org/bot{self.bot_token}/getMe"
        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            if data.get("ok"):
                self.logger.info("Telegram bot connection OK")
                return True
            else:
                self.logger.error("Telegram bot connection failed")
                return False
        except Exception as e:
            self.logger.error(f"Telegram bot connection error: {e}")
            return False

    def delayed_post(
        self,
        text: str,
        delay_sec: float,
        **kwargs
    ) -> Optional[int]:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π.
        """
        self.logger.info(f"Delaying message post for {delay_sec} seconds...")
        time.sleep(delay_sec)
        return self.send_text(text, **kwargs)

# –∫–æ–¥ - search_utils.py

import os
import json
import logging
from typing import List, Dict, Optional, Union, Tuple
from dataclasses import dataclass
from pathlib import Path
import uuid
from datetime import datetime

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Document:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    id: str
    content: str
    metadata: Dict = None
    timestamp: str = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()

@dataclass
class QueryResult:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
    document_id: str
    content: str
    similarity_score: float
    metadata: Dict
    embedding: List[float] = None

class DocumentProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        text = text.strip()
        text = ' '.join(text.split())  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text
    
    @staticmethod
    def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk)
                
        return chunks
    
    @classmethod
    def process_document(cls, content: str, chunk_size: int = 500) -> List[str]:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        cleaned = cls.clean_text(content)
        chunks = cls.chunk_text(cleaned, chunk_size)
        return chunks

class EmbeddingManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        logger.info(f"Loaded embedding model: {model_name}")
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.model.encode(texts, **kwargs)
        return embeddings
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        return cosine_similarity([embedding1], [embedding2])[0][0]

class AdvancedRAGPipeline:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π RAG pipeline"""
    
    def __init__(
        self,
        collection_name: str = "advanced_rag",
        persist_directory: Optional[str] = None,
        embedding_model: str = "all-MiniLM-L6-v2"
    ):
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
        self._init_chromadb()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.embedding_manager = EmbeddingManager(embedding_model)
        self.processor = DocumentProcessor()
        
        logger.info("RAG Pipeline initialized successfully")
    
    def _init_chromadb(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB"""
        settings = Settings()
        if self.persist_directory:
            settings.persist_directory = self.persist_directory
            Path(self.persist_directory).mkdir(parents=True, exist_ok=True)
        
        self.client = chromadb.Client(settings)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        try:
            self.collection = self.client.get_collection(self.collection_name)
            logger.info(f"Loaded existing collection: {self.collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "Advanced RAG Pipeline Collection"}
            )
            logger.info(f"Created new collection: {self.collection_name}")
    
    def add_document(
        self,
        content: str,
        doc_id: Optional[str] = None,
        metadata: Optional[Dict] = None,
        chunk_size: int = 500
    ) -> List[str]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É"""
        if doc_id is None:
            doc_id = str(uuid.uuid4())
        
        if metadata is None:
            metadata = {}
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunks = self.processor.process_document(content, chunk_size)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = self.embedding_manager.encode(chunks)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ChromaDB
        chunk_ids = []
        chunk_documents = []
        chunk_embeddings = []
        chunk_metadata = []
        
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_id = f"{doc_id}_chunk_{i}"
            chunk_ids.append(chunk_id)
            chunk_documents.append(chunk)
            chunk_embeddings.append(embedding.tolist())
            
            chunk_meta = {
                **metadata,
                "parent_doc_id": doc_id,
                "chunk_index": i,
                "chunk_count": len(chunks),
                "timestamp": datetime.now().isoformat()
            }
            chunk_metadata.append(chunk_meta)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
        self.collection.add(
            ids=chunk_ids,
            documents=chunk_documents,
            embeddings=chunk_embeddings,
            metadatas=chunk_metadata
        )
        
        logger.info(f"Added document {doc_id} with {len(chunks)} chunks")
        return chunk_ids
    
    def add_documents_batch(
        self,
        documents: List[Document],
        chunk_size: int = 500,
        batch_size: int = 100
    ) -> List[str]:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        all_chunk_ids = []
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_ids = []
            
            for doc in batch:
                chunk_ids = self.add_document(
                    content=doc.content,
                    doc_id=doc.id,
                    metadata=doc.metadata,
                    chunk_size=chunk_size
                )
                batch_ids.extend(chunk_ids)
            
            all_chunk_ids.extend(batch_ids)
            logger.info(f"Processed batch {i//batch_size + 1}: {len(batch)} documents")
        
        return all_chunk_ids
    
    def search(
        self,
        query: str,
        n_results: int = 5,
        filter_metadata: Optional[Dict] = None,
        min_similarity: float = 0.0
    ) -> List[QueryResult]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedding_manager.encode([query])[0]
        
        # –ü–æ–∏—Å–∫ –≤ ChromaDB
        where_clause = filter_metadata if filter_metadata else None
        
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=n_results,
            where=where_clause
        )
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        query_results = []
        
        for i in range(len(results['ids'][0])):
            doc_id = results['ids'][0][i]
            content = results['documents'][0][i]
            metadata = results['metadatas'][0][i]
            distance = results['distances'][0][i] if 'distances' in results else None
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ distance –≤ similarity score
            similarity_score = 1 - distance if distance is not None else 0.0
            
            if similarity_score >= min_similarity:
                query_result = QueryResult(
                    document_id=doc_id,
                    content=content,
                    similarity_score=similarity_score,
                    metadata=metadata
                )
                query_results.append(query_result)
        
        logger.info(f"Found {len(query_results)} results for query: '{query[:50]}...'")
        return query_results
    
    def delete_document(self, doc_id: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ—Ö –µ–≥–æ —á–∞–Ω–∫–æ–≤"""
        try:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            results = self.collection.get(
                where={"parent_doc_id": doc_id}
            )
            
            if results['ids']:
                self.collection.delete(ids=results['ids'])
                logger.info(f"Deleted document {doc_id} and {len(results['ids'])} chunks")
                return True
            else:
                logger.warning(f"Document {doc_id} not found")
                return False
                
        except Exception as e:
            logger.error(f"Error deleting document {doc_id}: {e}")
            return False
    
    def update_document(
        self,
        doc_id: str,
        new_content: str,
        new_metadata: Optional[Dict] = None,
        chunk_size: int = 500
    ) -> List[str]:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        self.delete_document(doc_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
        return self.add_document(
            content=new_content,
            doc_id=doc_id,
            metadata=new_metadata,
            chunk_size=chunk_size
        )
    
    def get_collection_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            count = self.collection.count()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            sample = self.collection.peek(limit=10)
            
            stats = {
                "total_chunks": count,
                "collection_name": self.collection_name,
                "sample_metadata": sample.get('metadatas', [])[:3] if sample else []
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting collection stats: {e}")
            return {"error": str(e)}
    
    def export_collection(self, filepath: str) -> bool:
        """–≠–∫—Å–ø–æ—Ä—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ JSON"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            all_data = self.collection.get()
            
            export_data = {
                "collection_name": self.collection_name,
                "export_timestamp": datetime.now().isoformat(),
                "data": {
                    "ids": all_data.get('ids', []),
                    "documents": all_data.get('documents', []),
                    "metadatas": all_data.get('metadatas', []),
                    "embeddings": all_data.get('embeddings', [])
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Collection exported to {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"Error exporting collection: {e}")
            return False
    
    def semantic_search_with_reranking(
        self,
        query: str,
        n_results: int = 10,
        rerank_top_k: int = 5
    ) -> List[QueryResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ü–µ—Ä–≤–∏—á–Ω—ã–π –ø–æ–∏—Å–∫
        initial_results = self.search(query, n_results=n_results)
        
        if not initial_results:
            return []
        
        # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        query_embedding = self.embedding_manager.encode([query])[0]
        
        reranked_results = []
        for result in initial_results:
            # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ
            doc_embedding = self.embedding_manager.encode([result.content])[0]
            similarity = self.embedding_manager.compute_similarity(
                query_embedding, doc_embedding
            )
            
            result.similarity_score = similarity
            reranked_results.append(result)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤—ã–º —Å–∫–æ—Ä–∞–º
        reranked_results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        return reranked_results[:rerank_top_k]

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG pipeline
    rag = AdvancedRAGPipeline(
        collection_name="advanced_rag_demo",
        persist_directory="./rag_storage"
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    test_documents = [
        Document(
            id="doc1",
            content="–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. ChromaDB - –æ–¥–Ω–∞ –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Python —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.",
            metadata={"category": "database", "language": "python"}
        ),
        Document(
            id="doc2", 
            content="RAG (Retrieval-Augmented Generation) pipeline –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤. –≠—Ç–æ –º–æ—â–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤.",
            metadata={"category": "ai", "topic": "rag"}
        ),
        Document(
            id="doc3",
            content="SentenceTransformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞. –ú–æ–¥–µ–ª—å all-MiniLM-L6-v2 –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á.",
            metadata={"category": "ml", "library": "sentence_transformers"}
        )
    ]
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    rag.add_documents_batch(test_documents)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
    stats = rag.get_collection_stats()
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    # –ü–æ–∏—Å–∫
    print("\n–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    query = "–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –≤ Python"
    results = rag.search(query, n_results=3)
    
    for i, result in enumerate(results):
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}:")
        print(f"ID: {result.document_id}")
        print(f"–°—Ö–æ–∂–µ—Å—Ç—å: {result.similarity_score:.3f}")
        print(f"–ö–æ–Ω—Ç–µ–Ω—Ç: {result.content[:100]}...")
        print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {result.metadata}")
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    print("\n–ü–æ–∏—Å–∫ —Å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º:")
    reranked = rag.semantic_search_with_reranking(query, n_results=5, rerank_top_k=2)
    
    for i, result in enumerate(reranked):
        print(f"\n–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç {i+1}:")
        print(f"–°—Ö–æ–∂–µ—Å—Ç—å: {result.similarity_score:.3f}")
        print(f"–ö–æ–Ω—Ç–µ–Ω—Ç: {result.content[:100]}...")

# –∫–æ–¥ - image_utils.py

import random
from pathlib import Path
from typing import Optional, Tuple, List

from PIL import Image, UnidentifiedImageError

# --- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã Telegram ---
SUPPORTED_IMAGE_EXTS = {".jpg", ".jpeg", ".png", ".gif", ".webp"}
SUPPORTED_VIDEO_EXTS = {".mp4", ".mov", ".mkv"}
SUPPORTED_DOC_EXTS   = {".pdf", ".docx", ".doc", ".txt", ".csv", ".xlsx"}
SUPPORTED_AUDIO_EXTS = {".mp3", ".wav", ".ogg"}
SUPPORTED_MEDIA_EXTS = SUPPORTED_IMAGE_EXTS | SUPPORTED_VIDEO_EXTS | SUPPORTED_DOC_EXTS | SUPPORTED_AUDIO_EXTS

# --- –†–∞–∑–º–µ—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
MAX_IMAGE_SIZE = (1280, 1280)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è Telegram (–ø–æ —Å—Ç–æ—Ä–æ–Ω–µ)
MAX_FILE_SIZE_MB = 50  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram –Ω–∞ —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (50 –ú–ë)

def is_safe_media_path(path: Path, media_dir: Path) -> bool:
    """–ü—É—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –≤–Ω—É—Ç—Ä–∏ media_dir –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –Ω–∞–≤–µ—Ä—Ö."""
    try:
        return media_dir.resolve(strict=False) in path.resolve(strict=False).parents or path.resolve() == media_dir.resolve(strict=False)
    except Exception:
        return False

def pick_random_media_file(media_dir: Path, allowed_exts: Optional[set] = None) -> Optional[Path]:
    """
    –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ media_dir (–≤–∫–ª—é—á–∞—è –ø–æ–¥–ø–∞–ø–∫–∏) —Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º.
    """
    if not media_dir.exists():
        return None
    allowed_exts = allowed_exts or SUPPORTED_MEDIA_EXTS
    files = [f for f in media_dir.rglob("*") if f.is_file() and f.suffix.lower() in allowed_exts]
    if not files:
        return None
    return random.choice(files)

def validate_media_file(path: Path, media_dir: Path = Path("media")) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –º–µ–¥–∏–∞-—Ñ–∞–π–ª–∞:
      - —Ç–æ–ª—å–∫–æ –∏–∑ –ø–∞–ø–∫–∏ media (–∏–ª–∏ –ø–æ–¥–ø–∞–ø–æ–∫)
      - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
      - –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞
      - —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    """
    if not path.exists():
        return False, "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"
    if not is_safe_media_path(path, media_dir):
        return False, "–§–∞–π–ª –≤–Ω–µ –ø–∞–ø–∫–∏ media"
    if path.suffix.lower() not in SUPPORTED_MEDIA_EXTS:
        return False, f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {path.suffix}"
    if path.stat().st_size > MAX_FILE_SIZE_MB * 1024 * 1024:
        return False, f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (>{MAX_FILE_SIZE_MB} –ú–ë)"
    return True, "OK"

def get_media_type(path: Path) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–µ–¥–∏–∞-—Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é.
    """
    ext = path.suffix.lower()
    if ext in SUPPORTED_IMAGE_EXTS:
        return "image"
    elif ext in SUPPORTED_VIDEO_EXTS:
        return "video"
    elif ext in SUPPORTED_DOC_EXTS:
        return "document"
    elif ext in SUPPORTED_AUDIO_EXTS:
        return "audio"
    return "unknown"

def process_image(path: Path, output_dir: Optional[Path] = None, max_size: Tuple[int,int]=MAX_IMAGE_SIZE) -> Optional[Path]:
    """
    –£–º–µ–Ω—å—à–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ max_size –ø–æ –±–æ–ª—å—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –Ω–æ–≤–æ–º—É —Ñ–∞–π–ª—É.
    """
    try:
        img = Image.open(path)
        img.thumbnail(max_size, Image.ANTIALIAS)
        out_dir = output_dir or path.parent
        out_path = out_dir / f"{path.stem}_resized{path.suffix}"
        img.save(out_path)
        return out_path
    except UnidentifiedImageError:
        return None
    except Exception:
        return None

def get_all_media_files(media_dir: Path, allowed_exts: Optional[set] = None) -> List[Path]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ media_dir (–∏ –ø–æ–¥–ø–∞–ø–∫–∞—Ö) —Å –Ω—É–∂–Ω—ã–º–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏.
    """
    allowed_exts = allowed_exts or SUPPORTED_MEDIA_EXTS
    return [f for f in media_dir.rglob("*") if f.is_file() and f.suffix.lower() in allowed_exts]

def prepare_media_for_post(media_dir: Path = Path("media")) -> Optional[Path]:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π —Ñ–∞–π–ª –∏–∑ media_dir.
    –ï—Å–ª–∏ —Ñ–∞–π–ª ‚Äî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None.
    """
    file = pick_random_media_file(media_dir)
    if not file:
        return None
    is_valid, reason = validate_media_file(file, media_dir)
    if not is_valid:
        return None
    media_type = get_media_type(file)
    if media_type == "image":
        # –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä, –µ—Å–ª–∏ –±–æ–ª—å—à–æ–µ ‚Äî —É–º–µ–Ω—å—à–∏–º
        img = Image.open(file)
        if img.size[0] > MAX_IMAGE_SIZE[0] or img.size[1] > MAX_IMAGE_SIZE[1]:
            resized = process_image(file)
            if resized is not None:
                return resized
    return file

# –∫–æ–¥ - rag_text_utils.py

from pathlib import Path
from typing import List, Union
from logs import get_logger

logger = get_logger("rag_text_utils")

# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–∞
COMMON_ENCODINGS = ["utf-8", "cp1251", "windows-1251", "latin-1", "iso-8859-1"]

def _smart_read_text(path: Path) -> str:
    """
    –ü—Ä–æ–±—É–µ—Ç –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∏–ª–∏ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç UnicodeDecodeError, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å.
    """
    for encoding in COMMON_ENCODINGS:
        try:
            return path.read_text(encoding=encoding)
        except Exception as e:
            logger.debug(f"–ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding} –¥–ª—è {path}: {e}")
    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {path} –≤ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∫–∞—Ö: {COMMON_ENCODINGS}")
    raise UnicodeDecodeError("all", b'', 0, 1, f"Failed to read {path} with encodings: {COMMON_ENCODINGS}")

def chunk_text(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 0,
    max_chunks: int = None
) -> List[str]:
    """
    –î–µ–ª–∏—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–ª–æ–≤–∞–º —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –∏ overlap.

    Args:
        text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.
        chunk_size (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —á–∞–Ω–∫–µ.
        overlap (int): –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏.
        max_chunks (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ (–æ–±—Ä–µ–∑–∫–∞).

    Returns:
        List[str]: –ß–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞.
    """
    if chunk_size <= 0:
        raise ValueError("chunk_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")
    if overlap < 0:
        raise ValueError("overlap –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º")
    words = text.split()
    if not words:
        return []
    chunks = []
    step = max(chunk_size - overlap, 1)
    for i in range(0, len(words), step):
        chunk = " ".join(words[i:i+chunk_size])
        if chunk:
            chunks.append(chunk)
        if max_chunks is not None and len(chunks) >= max_chunks:
            logger.info(f"–û–±—Ä–µ–∑–∞–Ω–æ –ø–æ max_chunks={max_chunks}")
            break
    return chunks

def process_text_file_for_rag(
    file_path: Path,
    chunk_size: int = 1000,
    overlap: int = 0,
    max_chunks: int = None,
    raise_on_error: bool = False
) -> List[str]:
    """
    –ß–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –∏ –¥–µ–ª–∏—Ç –µ–≥–æ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è RAG.

    Args:
        file_path (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        chunk_size (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —á–∞–Ω–∫–µ.
        overlap (int): –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏.
        max_chunks (int, optional): –ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –ø–∞–º—è—Ç—å.
        raise_on_error (bool): –ï—Å–ª–∏ True, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ, –∏–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [].

    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º –ø—Ä–∏ –æ—à–∏–±–∫–µ).
    """
    try:
        text = _smart_read_text(file_path)
        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap, max_chunks=max_chunks)
        logger.info(f"Text file processed for RAG: {file_path.name}, chunks: {len(chunks)}")
        return chunks
    except Exception as e:
        logger.error(f"process_text_file_for_rag error: {e}")
        if raise_on_error:
            raise
        return []

def process_text_for_rag(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 0,
    max_chunks: int = None
) -> List[str]:
    """
    –î–µ–ª–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è RAG.

    Args:
        text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.
        chunk_size (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —á–∞–Ω–∫–µ.
        overlap (int): –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏.
        max_chunks (int, optional): –ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤.

    Returns:
        List[str]: –ß–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞.
    """
    try:
        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap, max_chunks=max_chunks)
        logger.info(f"Arbitrary text processed for RAG, chunks: {len(chunks)}")
        return chunks
    except Exception as e:
        logger.error(f"process_text_for_rag error: {e}")
        return []

# –í–æ–∑–º–æ–∂–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: —á–∞–Ω–∫–∏–Ω–≥ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –∏ –∞–±–∑–∞—Ü–∞–º, –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —á—Ç–µ–Ω–∏–µ, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ stream-–æ–±—Ä–∞–±–æ—Ç–∫–∏.

# –∫–æ–¥ - rag_prompt_utils.py

from pathlib import Path
from typing import Optional, Union
from logs import get_logger

logger = get_logger("rag_prompt_utils")

def get_prompt_parts(
    data_dir: Union[str, Path],
    topic: str,
    context: str,
    uploadfile: Optional[Union[str, Path]] = None,
    file1: Optional[Union[str, Path]] = None,
    file2: Optional[Union[str, Path]] = None
) -> str:
    """
    –°–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤ –∏ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

    Args:
        data_dir (Union[str, Path]): –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —à–∞–±–ª–æ–Ω–∞–º–∏ prompt_1, prompt_2, prompt.txt.
        topic (str): –¢–µ–º–∞—Ç–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ {TOPIC}).
        context (str): –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ {CONTEXT}).
        uploadfile (Optional[Union[str, Path]]): –ü—É—Ç—å –∏–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ {UPLOADFILE}.
        file1 (Optional[Union[str, Path]]): –Ø–≤–Ω—ã–π –ø—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω).
        file2 (Optional[Union[str, Path]]): –Ø–≤–Ω—ã–π –ø—É—Ç—å –∫–æ –≤—Ç–æ—Ä–æ–º—É —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞.

    Returns:
        str: –ì–æ—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM —Å –ø–æ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ —à–∞–±–ª–æ–Ω–∞—Ö:
        - {TOPIC}: —Ç–µ–º–∞—Ç–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        - {CONTEXT}: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        - {UPLOADFILE}: –∏–º—è —Ñ–∞–π–ª–∞/—Å—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞ (—Å–º. uploadfile)
        - (–ú–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä {USER}, {DATE}, {EXTRA}, –µ—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –≤ –±—É–¥—É—â–µ–º)
    """
    import random

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –ø—É—Ç–µ–π –∫ Path –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    data_dir = Path(data_dir)
    if file1 is not None:
        file1 = Path(file1)
    if file2 is not None:
        file2 = Path(file2)
    if uploadfile is not None:
        uploadfile_path = Path(uploadfile)
    else:
        uploadfile_path = None

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è data_dir
    if not data_dir.exists() or not data_dir.is_dir():
        logger.error(f"data_dir '{data_dir}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π")
        return f"{topic}\n\n{context}"

    def read_template(path: Path) -> Optional[str]:
        try:
            return path.read_text(encoding="utf-8")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ {path}: {e}")
            return None

    prompt1_dir = data_dir / "prompt_1"
    prompt2_dir = data_dir / "prompt_2"
    template = None

    # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –ø–æ —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
    if file1 is not None and file2 is not None and file1.exists() and file2.exists():
        logger.info(f"–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω: {file1.name} + {file2.name}")
        txt1 = read_template(file1)
        txt2 = read_template(file2)
        if txt1 is not None and txt2 is not None:
            template = txt1 + "\n" + txt2
    # –°–ª—É—á–∞–π–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –∏–∑ prompt_1 –∏ prompt_2
    elif prompt1_dir.exists() and prompt2_dir.exists():
        prompt1_files = list(prompt1_dir.glob("*.txt"))
        prompt2_files = list(prompt2_dir.glob("*.txt"))
        if prompt1_files and prompt2_files:
            f1 = random.choice(prompt1_files)
            f2 = random.choice(prompt2_files)
            logger.info(f"–°–ª—É—á–∞–π–Ω—ã–π —à–∞–±–ª–æ–Ω: {f1.name} + {f2.name}")
            txt1 = read_template(f1)
            txt2 = read_template(f2)
            if txt1 is not None and txt2 is not None:
                template = txt1 + "\n" + txt2

    # Fallback –Ω–∞ prompt.txt
    if template is None:
        prompt_file = data_dir / "prompt.txt"
        if prompt_file.exists():
            logger.warning("Fallback –Ω–∞ prompt.txt")
            template = read_template(prompt_file)
        else:
            logger.warning("Fallback –Ω–∞ plain topic + context")
            return f"{topic}\n\n{context}"

    if template is None:
        logger.error("–ù–∏ –æ–¥–∏–Ω —à–∞–±–ª–æ–Ω –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å, –≤–æ–∑–≤—Ä–∞—Ç plain topic + context")
        return f"{topic}\n\n{context}"

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞ {UPLOADFILE}
    has_uploadfile = "{UPLOADFILE}" in template

    uploadfile_text = ""
    if has_uploadfile:
        if uploadfile_path is not None:
            try:
                if uploadfile_path.exists():
                    uploadfile_text = uploadfile_path.name
                    context = context[:1024]
                else:
                    uploadfile_text = f"[–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {uploadfile_path.name}]"
            except Exception as e:
                uploadfile_text = "[–û—à–∏–±–∫–∞ —Å —Ñ–∞–π–ª–æ–º]"
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ uploadfile: {e}")
        else:
            uploadfile_text = "[–§–∞–π–ª –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω]"

    if not has_uploadfile:
        context = context[:4096]

    # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
    prompt_out = (
        template.replace("{TOPIC}", topic)
                .replace("{CONTEXT}", context)
    )
    if has_uploadfile:
        prompt_out = prompt_out.replace("{UPLOADFILE}", uploadfile_text)
    # –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –±—É–¥—É—â–∏—Ö –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
    # –ù–∞–ø—Ä–∏–º–µ—Ä:
    # for placeholder, value in extra_placeholders.items():
    #     prompt_out = prompt_out.replace(f"{{{placeholder}}}", value)
    return prompt_out


# –∫–æ–¥ - RAG_Pipeline_Extensions_Utils.py

import os
import json
import csv
import requests
from typing import List, Dict, Optional, Generator
from pathlib import Path
import asyncio
import aiohttp
from dataclasses import dataclass
import xml.etree.ElementTree as ET
from urllib.parse import urlparse, urljoin
import time
import hashlib

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å PDF –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    print("BeautifulSoup4 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å DOCX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

from advanced_rag_pipeline import Document, AdvancedRAGPipeline

class DataIngestionManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self, rag_pipeline: AdvancedRAGPipeline):
        self.rag = rag_pipeline
    
    def load_from_text_file(self, filepath: str, encoding: str = 'utf-8') -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'r', encoding=encoding) as f:
                return f.read()
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {filepath}: {e}")
    
    def load_from_pdf(self, filepath: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        if not PDF_AVAILABLE:
            raise Exception("PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyPDF2")
        
        try:
            text = ""
            with open(filepath, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            return text
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {filepath}: {e}")
    
    def load_from_docx(self, filepath: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
        if not DOCX_AVAILABLE:
            raise Exception("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
        
        try:
            doc = docx.Document(filepath)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX {filepath}: {e}")
    
    def load_from_csv(self, filepath: str, text_columns: List[str]) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        try:
            documents = []
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for i, row in enumerate(reader):
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
                    content_parts = []
                    for col in text_columns:
                        if col in row and row[col]:
                            content_parts.append(str(row[col]))
                    
                    if content_parts:
                        content = " ".join(content_parts)
                        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        metadata = {k: v for k, v in row.items() if k not in text_columns}
                        
                        documents.append({
                            'id': f"csv_row_{i}",
                            'content': content,
                            'metadata': metadata
                        })
            
            return documents
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV {filepath}: {e}")
    
    def load_from_json(self, filepath: str, content_field: str, id_field: Optional[str] = None) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            documents = []
            if isinstance(data, list):
                for i, item in enumerate(data):
                    if content_field in item:
                        doc_id = item.get(id_field, f"json_item_{i}") if id_field else f"json_item_{i}"
                        content = str(item[content_field])
                        metadata = {k: v for k, v in item.items() if k not in [content_field, id_field]}
                        
                        documents.append({
                            'id': doc_id,
                            'content': content,
                            'metadata': metadata
                        })
            
            return documents
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON {filepath}: {e}")
    
    def load_from_url(self, url: str, timeout: int = 30) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å URL"""
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()
            
            # –ï—Å–ª–∏ —ç—Ç–æ HTML, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
            if 'text/html' in response.headers.get('content-type', ''):
                if BS4_AVAILABLE:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    # –£–¥–∞–ª—è–µ–º script –∏ style —Ç–µ–≥–∏
                    for script in soup(["script", "style"]):
                        script.decompose()
                    return soup.get_text()
                else:
                    return response.text
            else:
                return response.text
                
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL {url}: {e}")
    
    async def load_from_urls_async(self, urls: List[str], timeout: int = 30) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ URL"""
        async def fetch_url(session, url):
            try:
                async with session.get(url, timeout=timeout) as response:
                    content = await response.text()
                    if 'text/html' in response.headers.get('content-type', ''):
                        if BS4_AVAILABLE:
                            soup = BeautifulSoup(content, 'html.parser')
                            for script in soup(["script", "style"]):
                                script.decompose()
                            content = soup.get_text()
                    
                    return {
                        'id': hashlib.md5(url.encode()).hexdigest(),
                        'content': content,
                        'metadata': {'source_url': url, 'status': 'success'}
                    }
            except Exception as e:
                return {
                    'id': hashlib.md5(url.encode()).hexdigest(),
                    'content': "",
                    'metadata': {'source_url': url, 'status': 'error', 'error': str(e)}
                }
        
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_url(session, url) for url in urls]
            results = await asyncio.gather(*tasks)
            return [r for r in results if r['content']]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ

class RAGAnalytics:
    """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, rag_pipeline: AdvancedRAGPipeline):
        self.rag = rag_pipeline
        self.query_log = []
    
    def log_query(self, query: str, results_count: int, processing_time: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        self.query_log.append({
            'timestamp': time.time(),
            'query': query,
            'results_count': results_count,
            'processing_time': processing_time
        })
    
    def get_query_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º"""
        if not self.query_log:
            return {"message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º"}
        
        processing_times = [log['processing_time'] for log in self.query_log]
        results_counts = [log['results_count'] for log in self.query_log]
        
        return {
            'total_queries': len(self.query_log),
            'avg_processing_time': sum(processing_times) / len(processing_times),
            'max_processing_time': max(processing_times),
            'min_processing_time': min(processing_times),
            'avg_results_count': sum(results_counts) / len(results_counts),
            'queries_per_hour': len([q for q in self.query_log if time.time() - q['timestamp'] < 3600])
        }
    
    def analyze_collection_content(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            all_data = self.rag.collection.get()
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            
            if not documents:
                return {"message": "–ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞"}
            
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_docs = len(documents)
            total_chars = sum(len(doc) for doc in documents)
            avg_doc_length = total_chars / total_docs
            
            # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            categories = {}
            languages = {}
            
            for metadata in metadatas:
                if metadata:
                    if 'category' in metadata:
                        cat = metadata['category']
                        categories[cat] = categories.get(cat, 0) + 1
                    
                    if 'language' in metadata:
                        lang = metadata['language']
                        languages[lang] = languages.get(lang, 0) + 1
            
            return {
                'total_documents': total_docs,
                'total_characters': total_chars,
                'average_document_length': avg_doc_length,
                'categories_distribution': categories,
                'languages_distribution': languages,
                'longest_document': max(len(doc) for doc in documents),
                'shortest_document': min(len(doc) for doc in documents)
            }
            
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"}

class RAGWebInterface:
    """–ü—Ä–æ—Å—Ç–æ–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, rag_pipeline: AdvancedRAGPipeline, analytics: RAGAnalytics):
        self.rag = rag_pipeline
        self.analytics = analytics
    
    def generate_html_interface(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>RAG Pipeline Interface</title>
            <meta charset="UTF-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
                .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; }
                .search-box { width: 70%; padding: 10px; font-size: 16px; border: 1px solid #ddd; border-radius: 4px; }
                .search-btn { padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; }
                .result { margin: 15px 0; padding: 15px; border: 1px solid #ddd; border-radius: 4px; background: #f9f9f9; }
                .score { color: #666; font-size: 14px; }
                .metadata { color: #888; font-size: 12px; margin-top: 5px; }
                .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
                .stat-card { padding: 15px; background: #e9ecef; border-radius: 4px; text-align: center; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>üîç RAG Pipeline Interface</h1>
                
                <div class="search-section">
                    <input type="text" class="search-box" id="searchInput" placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å...">
                    <button class="search-btn" onclick="performSearch()">–ü–æ–∏—Å–∫</button>
                </div>
                
                <div id="results"></div>
                
                <h2>üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã</h2>
                <div class="stats" id="stats">
                    <div class="stat-card">
                        <h3>–ó–∞–ø—Ä–æ—Å—ã</h3>
                        <div id="queryCount">–ó–∞–≥—Ä—É–∑–∫–∞...</div>
                    </div>
                    <div class="stat-card">
                        <h3>–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è</h3>
                        <div id="avgTime">–ó–∞–≥—Ä—É–∑–∫–∞...</div>
                    </div>
                </div>
                
                <h2>üìà –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏</h2>
                <div style="margin: 20px 0;">
                    <button onclick="exportData()" style="margin-right: 10px; padding: 8px 16px; background: #28a745; color: white; border: none; border-radius: 4px;">–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö</button>
                    <button onclick="clearCollection()" style="padding: 8px 16px; background: #dc3545; color: white; border: none; border-radius: 4px;">–û—á–∏—Å—Ç–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é</button>
                </div>
            </div>
            
            <script>
                async function performSearch() {
                    const query = document.getElementById('searchInput').value;
                    if (!query.trim()) return;
                    
                    const resultsDiv = document.getElementById('results');
                    resultsDiv.innerHTML = '<div>–ü–æ–∏—Å–∫...</div>';
                    
                    try {
                        // –ó–¥–µ—Å—å –±—É–¥–µ—Ç API –≤—ã–∑–æ–≤ –∫ —Å–µ—Ä–≤–µ—Ä—É RAG
                        // –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        setTimeout(() => {
                            resultsDiv.innerHTML = `
                                <h3>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è: "${query}"</h3>
                                <div class="result">
                                    <strong>–î–æ–∫—É–º–µ–Ω—Ç 1</strong>
                                    <div class="score">–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.85</div>
                                    <p>–ü—Ä–∏–º–µ—Ä –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...</p>
                                    <div class="metadata">–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: category=ai, timestamp=2024-01-01</div>
                                </div>
                            `;
                        }, 1000);
                    } catch (error) {
                        resultsDiv.innerHTML = '<div style="color: red;">–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: ' + error.message + '</div>';
                    }
                }
                
                function loadStats() {
                    // –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    document.getElementById('docCount').textContent = '150';
                    document.getElementById('queryCount').textContent = '45';
                    document.getElementById('avgTime').textContent = '0.3s';
                }
                
                function exportData() {
                    alert('–§—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ');
                }
                
                function clearCollection() {
                    if (confirm('–í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ –æ—á–∏—Å—Ç–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é?')) {
                        alert('–§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ');
                    }
                }
                
                // –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                window.onload = loadStats;
                
                // –ü–æ–∏—Å–∫ –ø–æ Enter
                document.getElementById('searchInput').addEventListener('keypress', function(e) {
                    if (e.key === 'Enter') {
                        performSearch();
                    }
                });
            </script>
        </body>
        </html>
        """
        return html_template
    
    def save_interface(self, filepath: str = "rag_interface.html"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –≤ —Ñ–∞–π–ª"""
        html_content = self.generate_html_interface()
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"HTML –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")

class RAGBenchmarking:
    """–ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, rag_pipeline: AdvancedRAGPipeline):
        self.rag = rag_pipeline
    
    def create_test_dataset(self, size: int = 100) -> List[Document]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        test_topics = [
            "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
            "–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞",
            "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö",
            "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python", "–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"
        ]
        
        documents = []
        for i in range(size):
            topic = test_topics[i % len(test_topics)]
            content = self._generate_synthetic_content(topic, i)
            
            doc = Document(
                id=f"test_doc_{i}",
                content=content,
                metadata={
                    "topic": topic,
                    "test_id": i,
                    "synthetic": True
                }
            )
            documents.append(doc)
        
        return documents
    
    def _generate_synthetic_content(self, topic: str, doc_id: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤"""
        templates = [
            f"{topic} —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω–æ–π –æ–±–ª–∞—Å—Ç—å—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –î–æ–∫—É–º–µ–Ω—Ç –Ω–æ–º–µ—Ä {doc_id} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ {topic} –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö.",
            f"–í —ç—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã {topic}. –≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –Ω–æ–º–µ—Ä {doc_id} –ø–æ–º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏.",
            f"–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ {topic} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –≤ –ø—Ä–∏–º–µ—Ä–µ {doc_id}. –ó–¥–µ—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ –º–µ—Ç–æ–¥—ã."
        ]
        
        base_content = templates[doc_id % len(templates)]
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        additional = f" –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –≤–∫–ª—é—á–∞—é—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã, –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ {topic}."
        
        return base_content + additional
    
    def benchmark_search_performance(self, queries: List[str], n_runs: int = 10) -> Dict:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞"""
        results = {
            'queries': [],
            'avg_time': 0,
            'total_time': 0,
            'fastest_query': None,
            'slowest_query': None
        }
        
        total_time = 0
        fastest_time = float('inf')
        slowest_time = 0
        
        for query in queries:
            query_times = []
            
            for _ in range(n_runs):
                start_time = time.time()
                search_results = self.rag.search(query, n_results=5)
                end_time = time.time()
                
                query_time = end_time - start_time
                query_times.append(query_time)
                total_time += query_time
            
            avg_query_time = sum(query_times) / len(query_times)
            min_query_time = min(query_times)
            max_query_time = max(query_times)
            
            if min_query_time < fastest_time:
                fastest_time = min_query_time
                results['fastest_query'] = query
            
            if max_query_time > slowest_time:
                slowest_time = max_query_time
                results['slowest_query'] = query
            
            results['queries'].append({
                'query': query,
                'avg_time': avg_query_time,
                'min_time': min_query_time,
                'max_time': max_query_time,
                'runs': n_runs
            })
        
        results['avg_time'] = total_time / (len(queries) * n_runs)
        results['total_time'] = total_time
        results['fastest_time'] = fastest_time
        results['slowest_time'] = slowest_time
        
        return results
    
    def evaluate_retrieval_quality(self, test_queries: List[Dict]) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
        test_queries: [{'query': str, 'relevant_doc_ids': List[str]}]
        """
        metrics = {
            'precision_at_k': [],
            'recall_at_k': [],
            'average_precision': []
        }
        
        for test_case in test_queries:
            query = test_case['query']
            relevant_ids = set(test_case['relevant_doc_ids'])
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            results = self.rag.search(query, n_results=10)
            retrieved_ids = [r.document_id for r in results]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö K
            for k in [1, 3, 5, 10]:
                if k <= len(retrieved_ids):
                    retrieved_k = set(retrieved_ids[:k])
                    
                    # Precision@K
                    precision = len(retrieved_k & relevant_ids) / k
                    
                    # Recall@K
                    recall = len(retrieved_k & relevant_ids) / len(relevant_ids) if relevant_ids else 0
                    
                    metrics['precision_at_k'].append(precision)
                    metrics['recall_at_k'].append(recall)
            
            # Average Precision
            ap = self._calculate_average_precision(retrieved_ids, relevant_ids)
            metrics['average_precision'].append(ap)
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        final_metrics = {
            'mean_precision_at_k': sum(metrics['precision_at_k']) / len(metrics['precision_at_k']) if metrics['precision_at_k'] else 0,
            'mean_recall_at_k': sum(metrics['recall_at_k']) / len(metrics['recall_at_k']) if metrics['recall_at_k'] else 0,
            'mean_average_precision': sum(metrics['average_precision']) / len(metrics['average_precision']) if metrics['average_precision'] else 0
        }
        
        return final_metrics
    
    def _calculate_average_precision(self, retrieved_ids: List[str], relevant_ids: set) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Average Precision"""
        if not relevant_ids:
            return 0.0
        
        precisions = []
        relevant_count = 0
        
        for i, doc_id in enumerate(retrieved_ids):
            if doc_id in relevant_ids:
                relevant_count += 1
                precision = relevant_count / (i + 1)
                precisions.append(precision)
        
        return sum(precisions) / len(relevant_ids) if precisions else 0.0

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã
class RAGConfigManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, config_path: str = "rag_config.json"):
        self.config_path = config_path
        self.default_config = {
            "embedding_model": "all-MiniLM-L6-v2",
            "chunk_size": 500,
            "chunk_overlap": 50,
            "collection_name": "default_rag",
            "persist_directory": "./rag_storage",
            "search_results_limit": 5,
            "similarity_threshold": 0.3,
            "batch_size": 100,
            "logging_level": "INFO"
        }
        self.config = self.load_config()
    
    def load_config(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                for key, value in self.default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            else:
                return self.default_config.copy()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return self.default_config.copy()
    
    def save_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    
    def update_config(self, updates: Dict):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        self.config.update(updates)
        self.save_config()
    
    def get(self, key: str, default=None):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return self.config.get(key, default)

# –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
class FullRAGSystem:
    """–ü–æ–ª–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å–æ –≤—Å–µ–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self, config_path: str = "rag_config.json"):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.config_manager = RAGConfigManager(config_path)
        config = self.config_manager.config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        self.rag = AdvancedRAGPipeline(
            collection_name=config['collection_name'],
            persist_directory=config['persist_directory'],
            embedding_model=config['embedding_model']
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.data_manager = DataIngestionManager(self.rag)
        self.analytics = RAGAnalytics(self.rag)
        self.web_interface = RAGWebInterface(self.rag, self.analytics)
        self.benchmarking = RAGBenchmarking(self.rag)
        
        print("üöÄ –ü–æ–ª–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
    
    def ingest_directory(self, directory_path: str, file_extensions: List[str] = None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        if file_extensions is None:
            file_extensions = ['.txt', '.pdf', '.docx', '.json', '.csv']
        
        directory = Path(directory_path)
        if not directory.exists():
            raise Exception(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        
        documents = []
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in file_extensions:
                try:
                    if file_path.suffix.lower() == '.txt':
                        content = self.data_manager.load_from_text_file(str(file_path))
                        doc = Document(
                            id=str(file_path.stem),
                            content=content,
                            metadata={'source_file': str(file_path), 'file_type': 'text'}
                        )
                        documents.append(doc)
                    
                    elif file_path.suffix.lower() == '.pdf' and PDF_AVAILABLE:
                        content = self.data_manager.load_from_pdf(str(file_path))
                        doc = Document(
                            id=str(file_path.stem),
                            content=content,
                            metadata={'source_file': str(file_path), 'file_type': 'pdf'}
                        )
                        documents.append(doc)
                    
                    elif file_path.suffix.lower() == '.docx' and DOCX_AVAILABLE:
                        content = self.data_manager.load_from_docx(str(file_path))
                        doc = Document(
                            id=str(file_path.stem),
                            content=content,
                            metadata={'source_file': str(file_path), 'file_type': 'docx'}
                        )
                        documents.append(doc)
                    
                    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {file_path.name}")
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {e}")
        
        if documents:
            self.rag.add_documents_batch(documents)
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {directory_path}")
        else:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
    
    def create_web_interface(self, output_path: str = "rag_interface.html"):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        self.web_interface.save_interface(output_path)
        print(f"üåê –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–æ–∑–¥–∞–Ω: {output_path}")
    
    def run_performance_test(self):
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        test_queries = [
            "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö",
            "–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
        ]
        
        print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        results = self.benchmarking.benchmark_search_performance(test_queries)
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['avg_time']:.3f}s")
        print(f"   –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –∑–∞–ø—Ä–æ—Å: {results['fastest_query']} ({results['fastest_time']:.3f}s)")
        print(f"   –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {results['slowest_query']} ({results['slowest_time']:.3f}s)")
        
        return results
    
    def get_system_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        collection_stats = self.rag.get_collection_stats()
        query_stats = self.analytics.get_query_stats()
        content_analysis = self.analytics.analyze_collection_content()
        
        return {
            'collection_stats': collection_stats,
            'query_stats': query_stats,
            'content_analysis': content_analysis,
            'config': self.config_manager.config
        }

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
if __name__ == "__main__":
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã...")
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é —Å–∏—Å—Ç–µ–º—É
    full_system = FullRAGSystem()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_docs = [
        Document(
            id="advanced_doc_1",
            content="–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ RAG –≤–∫–ª—é—á–∞—é—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤, –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.",
            metadata={"category": "advanced", "topic": "rag_features"}
        ),
        Document(
            id="advanced_doc_2", 
            content="–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞, –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.",
            metadata={"category": "monitoring", "topic": "analytics"}
        )
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    full_system.rag.add_documents_batch(test_docs)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    print("\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞...")
    results = full_system.rag.search("–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RAG", n_results=2)
    for i, result in enumerate(results):
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}: {result.content[:100]}... (—Å—Ö–æ–∂–µ—Å—Ç—å: {result.similarity_score:.3f})")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
    print("\nüìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:")
    status = full_system.get_system_status()
    print(json.dumps(status, indent=2, ensure_ascii=False))
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    full_system.create_web_interface()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    performance_results = full_system.run_performance_test()
    
    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")card">
                        <h3>–î–æ–∫—É–º–µ–Ω—Ç—ã</h3>
                        <div id="docCount">–ó–∞–≥—Ä—É–∑–∫–∞...</div>
                    </div>
                    <div class="stat-


# –∫–æ–¥ - rag_lmclient.py

import re
import requests
import asyncio
from pathlib import Path
from typing import Optional, Callable, Any, Dict, List

# –í–ê–ñ–ù–û: enrich_context_with_tools –∏ get_prompt_parts –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è —è–≤–Ω–æ
from rag_langchain_tools import enrich_context_with_tools
from rag_utils import get_prompt_parts

class LMClient:
    def __init__(
        self,
        retriever,
        data_dir,
        inform_dir,
        logger,
        # a) –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ‚Äî —Ç–µ–ø–µ—Ä—å —è–≤–Ω—ã–µ, —Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏ –∏–ª–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ
        model_url: str,
        model_name: str,
        max_tokens: int = 1024,
        max_chars: int = 2600,
        max_attempts: int = 3,
        temperature: float = 0.7,
        timeout: int = 40,
        history_lim: int = 3,
        system_msg: Optional[str] = None
    ):
        self.retriever = retriever
        self.data_dir = Path(data_dir)
        self.inform_dir = Path(inform_dir)
        self.logger = logger

        self.model_url = model_url
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.max_chars = max_chars
        self.max_attempts = max_attempts
        self.temperature = temperature
        self.timeout = timeout
        self.history_lim = history_lim
        # b) system_msg —Ç–µ–ø–µ—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä, –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è —Ä–æ–ª—å ‚Äî –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
        self.system_msg = system_msg or "–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±—Ä–æ–≤—è–º –∏ —Ä–µ—Å–Ω–∏—Ü–∞–º."

    async def generate(self, topic: str, uploadfile: Optional[str] = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ —Ç–µ–º–µ —Å –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç/–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä/—Ç–∞–±–ª–∏—Ü–∞) –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
        uploadfile: –ø—É—Ç—å –∫ –ø—Ä–∏–∫—Ä–µ–ø–ª—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –¥–ª—è Telegram-–±–æ—Ç–∞ (–∏–ª–∏ None).
        """
        try:
            # 1. –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG.
            ctx = self.retriever.retrieve(topic)

            # 2. –û–±–æ–≥–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω—É–∂–Ω–æ.
            ctx = enrich_context_with_tools(topic, ctx, self.inform_dir)

            # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º—Ç (—Å–ª—É—á–∞–π–Ω–∞—è —Å–±–æ—Ä–∫–∞ prompt_1/prompt_2 –∏–ª–∏ fallback –Ω–∞ prompt.txt)
            try:
                user_text = get_prompt_parts(self.data_dir, topic, ctx, uploadfile=uploadfile)
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º—Ç–∞ –∏–∑ prompt_1/prompt_2: {e}")
                prompt_file = self.data_dir / 'prompt.txt'
                if not prompt_file.exists():
                    self.logger.error(f"Prompt file not found: {prompt_file}")
                    return "[–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω]"
                prompt_template = prompt_file.read_text(encoding='utf-8')
                user_text = prompt_template.replace('{TOPIC}', topic).replace('{CONTEXT}', ctx)

            # b) system message ‚Äî —Ç–µ–ø–µ—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä, –Ω–µ –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω
            system_msg = {"role": "system", "content": self.system_msg}
            user_msg = {"role": "user", "content": user_text}
            messages = [system_msg, user_msg]

            for attempt in range(self.max_attempts):
                try:
                    self.logger.info(f"Generation attempt {attempt + 1} for topic: {topic}")
                    resp = requests.post(
                        self.model_url,
                        json={
                            "model": self.model_name,
                            "messages": messages,
                            "temperature": self.temperature,
                            "max_tokens": self.max_tokens
                        },
                        timeout=self.timeout
                    )
                    resp.raise_for_status()
                    response_data = resp.json()
                    if 'choices' not in response_data or not response_data['choices']:
                        self.logger.error("Invalid LM response format")
                        continue
                    text = response_data['choices'][0]['message']['content'].strip()

                    # f) –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: —É–±–∏—Ä–∞–µ–º markdown-–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏, –ø—Ä–æ–º–æ-—Ç–µ–∫—Å—Ç—ã, —Å—Å—ã–ª–∫–∏
                    text = re.sub(r"(?m)^#{2,}.*$", "", text)  # markdown-–∑–∞–≥–æ–ª–æ–≤–∫–∏
                    text = re.sub(r"(?m)^---+", "", text)      # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                    text = re.sub(r"\[\[.*?\]\]\(.*?\)", "", text)  # markdown-—Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ [[1]](url)
                    text = re.sub(r"\n{2,}", "\n", text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫
                    # –£–¥–∞–ª—è–µ–º —è–≤–Ω—ã–µ —Ñ—Ä–∞–∑—ã LLM ("As an AI language model", "–Ø –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" –∏ —Ç.–ø.)
                    text = re.sub(
                        r"(as an ai language model|i am an ai language model|—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç|–∫–∞–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç)[\.,]?\s*",
                        "",
                        text, flags=re.IGNORECASE
                    )
                    text = text.strip()

                    if len(text) <= self.max_chars:
                        self.logger.info(f"Generated text length: {len(text)} chars")
                        return text
                    # e) –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
                    if attempt < self.max_attempts - 1:
                        messages.append({"role": "assistant", "content": text})
                        messages.append({
                            "role": "user",
                            "content": f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(text)}>{self.max_chars}), —Å–æ–∫—Ä–∞—Ç–∏ –¥–æ {self.max_chars} —Å–∏–º–≤–æ–ª–æ–≤."
                        })
                        sysm, rest = messages[0], messages[1:]
                        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ self.history_lim*2 —Å–æ–æ–±—â–µ–Ω–∏–π (user/assistant), –Ω–µ –Ω–∞—Ä—É—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                        last_msgs = []
                        for m in reversed(rest):
                            if len(last_msgs) >= self.history_lim * 2:
                                break
                            last_msgs.insert(0, m)
                        messages = [sysm] + last_msgs
                    else:
                        self.logger.warning(f"Force truncating text from {len(text)} to {self.max_chars} chars")
                        return text[:self.max_chars-10] + "..."
                except requests.exceptions.RequestException as e:
                    self.logger.error(f"LM request error on attempt {attempt + 1}: {e}")
                    # g) –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ notify_admin)
                    if attempt == self.max_attempts - 1:
                        # self.notify_admin(f"LM request failed after all attempts: {e}")  # –µ—Å–ª–∏ –µ—Å—Ç—å notify_admin
                        return "[–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é]"
                    await asyncio.sleep(5)
                except Exception as e:
                    self.logger.error(f"Unexpected error in generation attempt {attempt + 1}: {e}")
                    if attempt == self.max_attempts - 1:
                        # self.notify_admin(f"LM unexpected error after all attempts: {e}")
                        return "[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞]"
            return "[–û—à–∏–±–∫–∞: –ø—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏]"
        except Exception as e:
            self.logger.error(f"Critical error in generate: {e}")
            # self.notify_admin(f"Critical error in LMClient.generate: {e}")
            return "[–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏]"

# –∫–æ–¥ - rag_langchain_tools.py

import logging
from rag_utils import web_search, safe_eval, analyze_table
from pathlib import Path
from typing import Optional, Dict, Any, List

# –ü—Ä–∏—á–∏–Ω–∞: –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–ª–∞–¥–∫–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ—è,
# –∞ —Ç–∞–∫–∂–µ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –æ—Å—Ç–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π.
logger = logging.getLogger("rag_langchain_tools")
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] [rag_langchain_tools] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—à–∏—Ä—è–µ–º—ã–π –∫–æ–Ω—Ñ–∏–≥)
TOOL_KEYWORDS = {
    "web": ["–Ω–∞–π–¥–∏", "–ø–æ–∏—Å–∫", "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "lookup", "search", "google", "bing", "duckduckgo"],
    "calc": ["–≤—ã–≥–æ–¥", "–ø–æ—Å—á–∏—Ç", "calculate", "profit", "–≤—ã–±–µ—Ä–∏", "—Å–∫–æ–ª—å–∫–æ", "—Ä–∞—Å—Å—á–∏—Ç–∞–π"],
    "table": ["—Ç–∞–±–ª–∏—Ü", "excel", "csv", "xlsx", "–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π", "–¥–∞–Ω–Ω—ã–µ", "–æ—Ç—á–µ—Ç", "—Ç–∞–±–ª–∏—Ü–∞"]
}

def tool_internet_search(query: str, num_results: int = 8) -> str:
    logger.info(f"–í—ã–∑–æ–≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
    results = web_search(query, num_results=num_results)
    if not results:
        logger.warning("–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return "[–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤]"
    return "\n".join(results)

def tool_calculator(expr: str, variables: Optional[Dict[str, Any]] = None) -> str:
    logger.info(f"–í—ã–∑–æ–≤ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ —Å –≤—ã—Ä–∞–∂–µ–Ω–∏–µ–º: {expr}")
    try:
        return str(safe_eval(expr, variables=variables))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–∞–ª—å–∫—É–ª—è—Ü–∏–∏: {e}")
        return f"[–û—à–∏–±–∫–∞ –∫–∞–ª—å–∫—É–ª—è—Ü–∏–∏]: {e}"

def tool_table_analysis(
    table_filename: str,
    info_query: Optional[dict]=None,
    inform_dir: Optional[str]=None,
    max_rows: int = 18,
    max_cols: int = 10
) -> str:
    logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã: {table_filename}")
    try:
        file_path = Path(inform_dir) / table_filename
        return analyze_table(file_path, info_query, max_rows=max_rows, max_cols=max_cols)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        return f"[–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–∞–±–ª–∏—Ü—ã]: {e}"

def smart_tool_selector(
    topic: str,
    context: str,
    inform_dir: str,
    tool_keywords: Optional[Dict[str, List[str]]] = None,
    tool_log: Optional[List[str]] = None,
    max_tool_results: int = 8
) -> str:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–º—Ç, –≤—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã,
    –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ü–µ–ø–æ—á–∫–∏, –ª–æ–≥–∏—Ä—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è, —Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
    """
    tool_keywords = tool_keywords or TOOL_KEYWORDS
    tool_log = tool_log or []
    topic_lc = topic.lower()
    results = []
    used_tools = []

    # –ü—Ä–∏—á–∏–Ω–∞: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ ‚Äî –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
    # –°–Ω–∞—á–∞–ª–∞ web search
    if any(x in topic_lc for x in tool_keywords["web"]):
        logger.info("[smart_tool_selector] Web search triggered")
        tool_log.append("web_search")
        results.append("[–ò–Ω—Ç–µ—Ä–Ω–µ—Ç]:\n" + tool_internet_search(topic, num_results=max_tool_results))
        used_tools.append("web_search")
    # –ó–∞—Ç–µ–º –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä
    if any(x in topic_lc for x in tool_keywords["calc"]):
        import re
        logger.info("[smart_tool_selector] Calculator triggered")
        tool_log.append("calculator")
        m = re.search(r"(–ø–æ—Å—á–∏—Ç–∞–π|calculate|–≤—ã–≥–æ–¥–Ω–µ–µ|–≤—ã–≥–æ–¥–Ω–æ—Å—Ç—å|—Å–∫–æ–ª—å–∫–æ)[^\d]*(.+)", topic_lc)
        expr = m.group(2) if m else topic
        results.append("[–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä]:\n" + tool_calculator(expr))
        used_tools.append("calculator")
    # –ó–∞—Ç–µ–º —Ç–∞–±–ª–∏—Ü–∞
    if any(x in topic_lc for x in tool_keywords["table"]):
        logger.info("[smart_tool_selector] Table analysis triggered")
        tool_log.append("analyze_table")
        table_files = [f.name for f in Path(inform_dir).glob("*.csv")] + [f.name for f in Path(inform_dir).glob("*.xlsx")]
        if table_files:
            results.append("[–¢–∞–±–ª–∏—Ü–∞]:\n" + tool_table_analysis(table_files[0], None, inform_dir))
            used_tools.append("analyze_table")
        else:
            results.append("[–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞]")

    if used_tools:
        logger.info(f"–í—ã–∑–≤–∞–Ω—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: {used_tools}")
    if results:
        return "\n\n".join(results)
    else:
        logger.info("–ù–∏ –æ–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –Ω–µ –±—ã–ª –≤—ã–∑–≤–∞–Ω")
        return ""

def enrich_context_with_tools(
    topic: str,
    context: str,
    inform_dir: str,
    max_tool_results: int = 8
) -> str:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –º–∞–∫—Å–∏–º—É–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    logger.info("–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏...")
    tool_result = smart_tool_selector(topic, context, inform_dir, max_tool_results=max_tool_results)
    if tool_result:
        context = context + "\n\n[–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ]:\n" + tool_result
        logger.info("–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.")
    else:
        logger.info("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –Ω–µ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.")
    return context

# –∫–æ–¥ - utils/config_manager.py
from pathlib import Path
import json
from typing import Any, Dict, Optional
import logging

class ConfigManager:
    def __init__(self, config_path: Path):
        self.config_path = config_path
        self.logger = logging.getLogger("config_manager")
        self.config = self._load_config()
        self._validate_config()

    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.critical(f"Failed to load configuration: {e}")
            raise

    def _validate_config(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        required_sections = ['telegram', 'language_model', 'retrieval', 'system', 'paths']
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"Missing required config section: {section}")

    def get_telegram_config(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Telegram"""
        config = self.config['telegram'].copy()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞ –∏ channel_id –∏–∑ —Ñ–∞–π–ª–æ–≤
        try:
            token_file = Path(config['bot_token_file'])
            channel_file = Path(config['channel_id_file'])
            
            if not token_file.exists():
                raise ValueError(f"Telegram token file not found: {token_file}")
            if not channel_file.exists():
                raise ValueError(f"Channel ID file not found: {channel_file}")
            
            config['bot_token'] = token_file.read_text(encoding='utf-8').strip()
            config['channel_id'] = channel_file.read_text(encoding='utf-8').strip()
            
            return config
        except Exception as e:
            self.logger.critical(f"Failed to load Telegram credentials: {e}")
            raise

    def get_path(self, path_key: str) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if path_key not in self.config['paths']:
            raise KeyError(f"Path not found in config: {path_key}")
        return Path(self.config['paths'][path_key])

    def get(self, section: str, key: str, default: Any = None) -> Any:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return self.config.get(section, {}).get(key, default)

    def update(self, section: str, key: str, value: Any):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if section not in self.config:
            self.config[section] = {}
        self.config[section][key] = value
        self._save_config()

    def _save_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=4, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"Failed to save configuration: {e}")
            raise

# –∫–æ–¥ -  utils/exceptions.py
class RAGException(Exception):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏–π RAG —Å–∏—Å—Ç–µ–º—ã"""
    pass

class ConfigurationError(RAGException):
    """–û—à–∏–±–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    pass

class InitializationError(RAGException):
    """–û—à–∏–±–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    pass

class ProcessingError(RAGException):
    """–û—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    pass

class ModelError(RAGException):
    """–û—à–∏–±–∫–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    pass

class TelegramError(RAGException):
    """–û—à–∏–±–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Telegram"""
    pass

class FileOperationError(RAGException):
    """–û—à–∏–±–∫–∏ —Ñ–∞–π–ª–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
    pass


# –∫–æ–¥ - utils/state_manager.py
from pathlib import Path
import json
from typing import Dict, Any, Set
from datetime import datetime
import logging

class StateManager:
    def __init__(self, state_file: Path):
        self.state_file = state_file
        self.logger = logging.getLogger("state_manager")
        self.state: Dict[str, Any] = self._load_state()

    def _load_state(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"Failed to load state: {e}")
                return self._create_default_state()
        return self._create_default_state()

    def _create_default_state(self) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "last_update": datetime.utcnow().isoformat(),
            "processed_topics": set(),
            "failed_topics": {},
            "statistics": {
                "total_processed": 0,
                "successful": 0,
                "failed": 0
            }
        }

    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list –¥–ª—è JSON
            state_copy = self.state.copy()
            state_copy["processed_topics"] = list(self.state["processed_topics"])
            state_copy["last_update"] = datetime.utcnow().isoformat()

            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state_copy, f, indent=4, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"Failed to save state: {e}")
            raise

    def add_processed_topic(self, topic: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Ç–µ–º—ã"""
        self.state["processed_topics"].add(topic)
        self.state["statistics"]["total_processed"] += 1
        self.state["statistics"]["successful"] += 1
        self.save_state()

    def add_failed_topic(self, topic: str, error: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–º—ã —Å –æ—à–∏–±–∫–æ–π"""
        self.state["failed_topics"][topic] = {
            "error": error,
            "timestamp": datetime.utcnow().isoformat(),
            "attempts": self.state["failed_topics"].get(topic, {}).get("attempts", 0) + 1
        }
        self.state["statistics"]["failed"] += 1
        self.save_state()

    def get_processed_topics(self) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–º"""
        return self.state["processed_topics"]

    def get_failed_topics(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–µ–º —Å –æ—à–∏–±–∫–∞–º–∏"""
        return self.state["failed_topics"]

    def get_statistics(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return self.state["statistics"]

    def clear_failed_topics(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–º —Å –æ—à–∏–±–∫–∞–º–∏"""
        self.state["failed_topics"].clear()
        self.save_state()

# –∫–æ–¥ - utils/file_manager.py

from pathlib import Path
import shutil
import tempfile
from typing import Optional, List
import logging
from datetime import datetime, timedelta

class TempFileManager:
    def __init__(self, temp_dir: Path):
        self.temp_dir = temp_dir
        self.logger = logging.getLogger("temp_file_manager")
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
    def create_temp_copy(self, source: Path) -> Optional[Path]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–ø–∏–∏ —Ñ–∞–π–ª–∞"""
        if not source.exists():
            return None
        
        try:
            temp_file = self.temp_dir / f"temp_{datetime.utcnow().timestamp()}_{source.name}"
            shutil.copy2(source, temp_file)
            return temp_file
        except Exception as e:
            self.logger.error(f"Failed to create temp copy of {source}: {e}")
            return None

    def cleanup_old_files(self, max_age_hours: int = 24):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
        
        try:
            for file in self.temp_dir.glob("temp_*"):
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º timestamp –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    timestamp = float(file.name.split('_')[1])
                    file_time = datetime.fromtimestamp(timestamp)
                    
                    if file_time < cutoff_time:
                        file.unlink()
                except (ValueError, IndexError):
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å timestamp, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª
                    continue
        except Exception as e:
            self.logger.error(f"Error during cleanup: {e}")

    def get_temp_file_path(self, prefix: str = "") -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        return self.temp_dir / f"temp_{prefix}_{datetime.utcnow().timestamp()}"

    def cleanup_file(self, file_path: Optional[Path]):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        if file_path and file_path.exists() and file_path.parent == self.temp_dir:
            try:
                file_path.unlink()
            except Exception as e:
                self.logger.error(f"Failed to cleanup file {file_path}: {e}")

